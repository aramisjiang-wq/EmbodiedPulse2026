# Embodied Pulse - æŠ€æœ¯è§„æ ¼æ–‡æ¡£ (Spec)

**æ–‡æ¡£ç‰ˆæœ¬**: v2.0  
**åˆ›å»ºæ—¥æœŸ**: 2025-12-08  
**æœ€åæ›´æ–°**: 2025-12-19  
**ç³»ç»ŸçŠ¶æ€**: âœ… ç”Ÿäº§ç¯å¢ƒè¿è¡Œä¸­  
**äº§å“åç§°**: Embodied Pulse (åŸ Robotics ArXiv Daily)

---

## ğŸ“‹ æ–‡æ¡£è¯´æ˜

æœ¬æ–‡æ¡£æ˜¯ Embodied Pulseï¼ˆåŸ Robotics ArXiv Dailyï¼‰é¡¹ç›®çš„æŠ€æœ¯è§„æ ¼æ–‡æ¡£ï¼ˆTechnical Specificationï¼‰ï¼Œè¯¦ç»†æè¿°äº†ç³»ç»Ÿæ¶æ„ã€æŠ€æœ¯å®ç°ã€APIè§„èŒƒå’Œéƒ¨ç½²æ–¹æ¡ˆã€‚

**æ–‡æ¡£ç›®æ ‡**ï¼š
- å®šä¹‰ç³»ç»Ÿæ¶æ„å’ŒæŠ€æœ¯é€‰å‹
- è§„èŒƒAPIæ¥å£å’Œæ•°æ®æ¨¡å‹
- æŒ‡å¯¼æŠ€æœ¯å®ç°å’Œéƒ¨ç½²
- ä½œä¸ºæŠ€æœ¯å›¢é˜Ÿçš„å‚è€ƒæ–‡æ¡£

---

## 1. ç³»ç»Ÿæ¶æ„

### 1.1 æ€»ä½“æ¶æ„

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    Client Layer (å®¢æˆ·ç«¯å±‚)                â”‚
â”‚                                                           â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”        â”‚
â”‚  â”‚  Web Browser    â”‚         â”‚   API Client    â”‚        â”‚
â”‚  â”‚  (HTML/CSS/JS)  â”‚         â”‚   (REST API)    â”‚        â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜         â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜        â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                            â†“ HTTP/HTTPS
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                Application Layer (åº”ç”¨å±‚)                 â”‚
â”‚                                                           â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚  â”‚              Flask Web Application                 â”‚  â”‚
â”‚  â”‚                                                     â”‚  â”‚
â”‚  â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”‚  â”‚
â”‚  â”‚  â”‚ Routes   â”‚  â”‚ Business â”‚  â”‚  API Handlersâ”‚    â”‚  â”‚
â”‚  â”‚  â”‚ (è·¯ç”±)   â”‚â†’ â”‚ Logic    â”‚â†’ â”‚  (æ¥å£å¤„ç†)  â”‚    â”‚  â”‚
â”‚  â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚ (ä¸šåŠ¡é€»è¾‘)â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â”‚  â”‚
â”‚  â”‚                â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                        â”‚  â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
â”‚                            â†“                              â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚  â”‚           Background Services (åå°æœåŠ¡)           â”‚  â”‚
â”‚  â”‚                                                     â”‚  â”‚
â”‚  â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”       â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”      â”‚  â”‚
â”‚  â”‚  â”‚  Scheduler   â”‚       â”‚  Fetch Worker   â”‚      â”‚  â”‚
â”‚  â”‚  â”‚  (å®šæ—¶ä»»åŠ¡)  â”‚â”€â”€â”€â”€â”€â”€â†’â”‚  (æŠ“å–å·¥ä½œè€…)   â”‚      â”‚  â”‚
â”‚  â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜       â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜      â”‚  â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                            â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                  Data Layer (æ•°æ®å±‚)                      â”‚
â”‚                                                           â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”      â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚  â”‚  SQLAlchemy ORM  â”‚      â”‚   Direct DB Access      â”‚  â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜      â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
â”‚           â”‚                            â”‚                 â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚  â”‚          PostgreSQL Database (robotics_arxiv)      â”‚  â”‚
â”‚  â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚  â”‚
â”‚  â”‚  â”‚  Table: papers                               â”‚  â”‚  â”‚
â”‚  â”‚  â”‚  - id (PK), title, authors, dates           â”‚  â”‚  â”‚
â”‚  â”‚  â”‚  - category, urls, abstract                  â”‚  â”‚  â”‚
â”‚  â”‚  â”‚  Indexes: category, publish_date, title     â”‚  â”‚  â”‚
â”‚  â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚  â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
â”‚                            â”‚                              â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚  â”‚        Backup: JSON Files (docs/*.json)           â”‚  â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                            â†‘
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚              External Services (å¤–éƒ¨æœåŠ¡)                 â”‚
â”‚                                                           â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                                â”‚
â”‚  â”‚     ArXiv API        â”‚                                â”‚
â”‚  â”‚  (è®ºæ–‡æ•°æ®æº)         â”‚                                â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                                â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### 1.2 æŠ€æœ¯æ ˆ

#### å‰ç«¯æŠ€æœ¯æ ˆ
- **HTML5**: é¡µé¢ç»“æ„
- **CSS3**: æ ·å¼è®¾è®¡ï¼ˆå“åº”å¼ã€åŠ¨ç”»ã€æ¸å˜ï¼‰
- **JavaScript (ES6+)**: äº¤äº’é€»è¾‘
- **Font Awesome 6.4**: å›¾æ ‡åº“
- **æ— æ¡†æ¶**: åŸç”ŸJavaScriptå®ç°

#### åç«¯æŠ€æœ¯æ ˆ
- **Python 3.9+**: ç¼–ç¨‹è¯­è¨€
- **Flask 3.0+**: Webæ¡†æ¶
- **SQLAlchemy 2.0+**: ORMæ¡†æ¶
- **PostgreSQL 15+**: å…³ç³»å‹æ•°æ®åº“ï¼ˆv1.3.0å·²å…¨é¢å‡çº§ï¼‰
- **psycopg2-binary**: PostgreSQLé©±åŠ¨
- **arxiv**: ArXiv APIå®¢æˆ·ç«¯
- **APScheduler 3.10+**: å®šæ—¶ä»»åŠ¡è°ƒåº¦
- **PyYAML**: é…ç½®æ–‡ä»¶è§£æ
- **requests**: HTTPè¯·æ±‚åº“
- **Gunicorn**: WSGI HTTPæœåŠ¡å™¨ï¼ˆç”Ÿäº§ç¯å¢ƒï¼‰
- **semantic-scholar-api**: Semantic Scholar APIå®¢æˆ·ç«¯

#### æ•°æ®å­˜å‚¨
- **PostgreSQL 15+**: å…³ç³»å‹æ•°æ®åº“ï¼ˆv1.3.0å·²å‡çº§ï¼‰
  - æ”¯æŒè¿æ¥æ± ï¼ˆpool_size=10, max_overflow=20ï¼‰
  - è‡ªåŠ¨é‡è¿æœºåˆ¶ï¼ˆpool_pre_ping=Trueï¼‰
  - æ”¯æŒé«˜å¹¶å‘è¯»å†™
- **SQLite 3**: å‘åå…¼å®¹ï¼ˆé€šè¿‡ç¯å¢ƒå˜é‡åˆ‡æ¢ï¼‰
- **JSON**: æ•°æ®å¤‡ä»½æ ¼å¼

#### å¼€å‘å·¥å…·
- **Git**: ç‰ˆæœ¬æ§åˆ¶
- **pip**: åŒ…ç®¡ç†
- **venv**: è™šæ‹Ÿç¯å¢ƒ

### 1.3 é¡¹ç›®ç»“æ„

```
robotics_arXiv_daily/
â”œâ”€â”€ app.py                      # Flaskåº”ç”¨ä¸»å…¥å£
â”œâ”€â”€ daily_arxiv.py              # è®ºæ–‡æŠ“å–æ ¸å¿ƒé€»è¾‘
â”œâ”€â”€ fetch_new_data.py           # ç»Ÿä¸€çš„æ•°æ®æŠ“å–å‡½æ•°
â”œâ”€â”€ fetch_news.py               # æ–°é—»æŠ“å–é€»è¾‘
â”œâ”€â”€ fetch_jobs.py               # æ‹›è˜ä¿¡æ¯æŠ“å–é€»è¾‘
â”œâ”€â”€ models.py                   # æ•°æ®åº“æ¨¡å‹å®šä¹‰
â”œâ”€â”€ news_models.py              # æ–°é—»æ•°æ®æ¨¡å‹
â”œâ”€â”€ jobs_models.py              # æ‹›è˜æ•°æ®æ¨¡å‹
â”œâ”€â”€ datasets_models.py          # æ•°æ®é›†æ•°æ®æ¨¡å‹
â”œâ”€â”€ utils.py                    # å·¥å…·å‡½æ•°
â”œâ”€â”€ save_paper_to_db.py         # è®ºæ–‡ä¿å­˜åˆ°æ•°æ®åº“
â”œâ”€â”€ save_news_to_db.py          # æ–°é—»ä¿å­˜åˆ°æ•°æ®åº“
â”œâ”€â”€ save_jobs_to_db.py          # æ‹›è˜ä¿¡æ¯ä¿å­˜åˆ°æ•°æ®åº“
â”œâ”€â”€ init_database.py            # æ•°æ®åº“åˆå§‹åŒ–
â”œâ”€â”€ migrate_json_to_db.py       # JSONæ•°æ®è¿ç§»
â”œâ”€â”€ gunicorn_config.py          # Gunicorné…ç½®æ–‡ä»¶ï¼ˆå«å®šæ—¶ä»»åŠ¡hookï¼‰
â”‚
â”œâ”€â”€ config.yaml                 # é…ç½®æ–‡ä»¶
â”œâ”€â”€ requirements.txt            # Pythonä¾èµ–
â”œâ”€â”€ start_web.sh                # å¯åŠ¨è„šæœ¬ï¼ˆæ™®é€šï¼‰
â”œâ”€â”€ start_with_auto_fetch.sh    # å¯åŠ¨è„šæœ¬ï¼ˆè‡ªåŠ¨æŠ“å–ï¼‰
â”‚
â”œâ”€â”€ templates/                  # HTMLæ¨¡æ¿
â”‚   â””â”€â”€ index.html              # ä¸»é¡µæ¨¡æ¿
â”‚
â”œâ”€â”€ static/                     # é™æ€èµ„æº
â”‚   â”œâ”€â”€ css/
â”‚   â”‚   â””â”€â”€ style.css           # æ ·å¼æ–‡ä»¶
â”‚   â”œâ”€â”€ js/
â”‚   â”‚   â””â”€â”€ app.js              # å‰ç«¯JavaScript
â”‚   â””â”€â”€ images/
â”‚       â””â”€â”€ banner-bg.webp      # BannerèƒŒæ™¯å›¾
â”‚
â”œâ”€â”€ docs/                       # æ–‡æ¡£å’Œæ•°æ®
â”‚   â”œâ”€â”€ cv-arxiv-daily.json     # JSONæ•°æ®å¤‡ä»½
â”‚   â”œâ”€â”€ cv-arxiv-daily-web.json # Webæ ¼å¼JSON
â”‚   â”œâ”€â”€ index.md                # GitHub Pagesé¦–é¡µ
â”‚   â””â”€â”€ é¡¹ç›®æ–‡æ¡£/               # é¡¹ç›®æ–‡æ¡£
â”‚       â”œâ”€â”€ PRD_äº§å“éœ€æ±‚æ–‡æ¡£_20251208.md
â”‚       â”œâ”€â”€ SPEC_æŠ€æœ¯è§„æ ¼æ–‡æ¡£_20251208.md
â”‚       â”œâ”€â”€ ä½¿ç”¨è¯´æ˜/
â”‚       â”œâ”€â”€ åŠŸèƒ½è¯´æ˜/
â”‚       â”œâ”€â”€ æŠ€æœ¯æ–‡æ¡£/
â”‚       â”œâ”€â”€ æµ‹è¯•æ–‡æ¡£/
â”‚       â”œâ”€â”€ é—®é¢˜ä¿®å¤/
â”‚       â”œâ”€â”€ ä¼˜åŒ–è®°å½•/
â”‚       â”œâ”€â”€ images/
â”‚       â””â”€â”€ æ²Ÿé€šè®°å½•/
â”‚
â”œâ”€â”€ migrate_sqlite_to_postgresql.py  # SQLiteåˆ°PostgreSQLè¿ç§»è„šæœ¬
â”œâ”€â”€ docker-compose.yml          # Docker Composeé…ç½®ï¼ˆå«PostgreSQLï¼‰
â”œâ”€â”€ docker-entrypoint.sh        # Dockerå®¹å™¨å¯åŠ¨è„šæœ¬
â”œâ”€â”€ Dockerfile                  # Dockeré•œåƒæ„å»ºæ–‡ä»¶
â”œâ”€â”€ gunicorn_config.py          # Gunicorné…ç½®ï¼ˆå«å®šæ—¶ä»»åŠ¡hookï¼‰
â”œâ”€â”€ semantic_scholar_client.py   # Semantic Scholar APIå®¢æˆ·ç«¯
â”œâ”€â”€ news_client.py              # æ–°é—»æŠ“å–å®¢æˆ·ç«¯
â”œâ”€â”€ github_jobs_client.py       # GitHub Jobs APIå®¢æˆ·ç«¯
â””â”€â”€ scripts/                    # éƒ¨ç½²å’Œå·¥å…·è„šæœ¬
```

---

## 2. æ•°æ®æ¨¡å‹

### 2.1 æ•°æ®åº“è®¾è®¡

#### Paper è¡¨ç»“æ„

```sql
CREATE TABLE papers (
    id TEXT PRIMARY KEY,              -- ArXiv ID (å¦‚: 2512.05107)
    title TEXT NOT NULL,              -- è®ºæ–‡æ ‡é¢˜
    authors TEXT,                     -- ä½œè€…åˆ—è¡¨ï¼ˆé€—å·åˆ†éš”ï¼‰
    publish_date DATE,                -- å‘å¸ƒæ—¥æœŸ
    update_date DATE,                 -- æ›´æ–°æ—¥æœŸ
    pdf_url TEXT,                     -- PDFé“¾æ¥
    code_url TEXT,                    -- ä»£ç é“¾æ¥ï¼ˆå¯é€‰ï¼‰
    abstract TEXT,                    -- æ‘˜è¦ï¼ˆå¯é€‰ï¼‰
    category TEXT,                    -- ç±»åˆ«
    created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
    updated_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP
);

-- ç´¢å¼•
CREATE INDEX idx_category ON papers(category);
CREATE INDEX idx_publish_date ON papers(publish_date);
CREATE INDEX idx_title ON papers(title);
```

#### å­—æ®µè¯´æ˜

| å­—æ®µ | ç±»å‹ | çº¦æŸ | è¯´æ˜ |
|------|------|------|------|
| id | TEXT | PRIMARY KEY | ArXivè®ºæ–‡IDï¼Œå”¯ä¸€æ ‡è¯† |
| title | TEXT | NOT NULL | è®ºæ–‡æ ‡é¢˜ |
| authors | TEXT | - | ä½œè€…åˆ—è¡¨ï¼Œé€—å·åˆ†éš” |
| publish_date | DATE | - | é¦–æ¬¡å‘å¸ƒæ—¥æœŸ |
| update_date | DATE | - | æœ€åæ›´æ–°æ—¥æœŸ |
| pdf_url | TEXT | - | ArXiv PDFé“¾æ¥ |
| code_url | TEXT | NULLABLE | GitHubç­‰ä»£ç é“¾æ¥ |
| abstract | TEXT | NULLABLE | è®ºæ–‡æ‘˜è¦ |
| category | TEXT | - | ç ”ç©¶ç±»åˆ«ï¼ˆ9ä¸ªç±»åˆ«ä¹‹ä¸€ï¼‰ |
| created_at | TIMESTAMP | DEFAULT NOW | è®°å½•åˆ›å»ºæ—¶é—´ |
| updated_at | TIMESTAMP | DEFAULT NOW | è®°å½•æ›´æ–°æ—¶é—´ |

#### ç ”ç©¶ç±»åˆ«æšä¸¾

```python
CATEGORIES = [
    'Perception',      # æ„ŸçŸ¥ç†è§£
    'VLM',            # è§†è§‰è¯­è¨€æ¨¡å‹
    'Planning',       # è§„åˆ’å†³ç­–
    'RL/IL',          # å¼ºåŒ–å­¦ä¹ /æ¨¡ä»¿å­¦ä¹ 
    'Manipulation',   # æœºå™¨äººæ“ä½œ
    'Locomotion',     # è¿åŠ¨æ§åˆ¶
    'Dexterous',      # çµå·§æ“ä½œ
    'VLA',            # è§†è§‰è¯­è¨€åŠ¨ä½œ
    'Humanoid'        # äººå½¢æœºå™¨äºº
]
```

### 2.2 æ•°æ®æ¨¡å‹ï¼ˆORMï¼‰

```python
from sqlalchemy import Column, String, Date, Text, DateTime, Index
from sqlalchemy.ext.declarative import declarative_base
from datetime import datetime

Base = declarative_base()

class Paper(Base):
    """è®ºæ–‡æ¨¡å‹"""
    __tablename__ = 'papers'
    
    id = Column(String, primary_key=True)
    title = Column(Text, nullable=False)
    authors = Column(Text)
    publish_date = Column(Date)
    update_date = Column(Date)
    pdf_url = Column(Text)
    code_url = Column(Text, nullable=True)
    abstract = Column(Text, nullable=True)
    category = Column(String)
    created_at = Column(DateTime, default=datetime.now)
    updated_at = Column(DateTime, default=datetime.now, onupdate=datetime.now)
    
    __table_args__ = (
        Index('idx_category', 'category'),
        Index('idx_publish_date', 'publish_date'),
        Index('idx_title', 'title'),
    )
    
    def to_dict(self):
        """è½¬æ¢ä¸ºå­—å…¸"""
        return {
            'id': self.id,
            'title': self.title,
            'authors': self.authors,
            'date': self.publish_date.strftime('%Y-%m-%d') if self.publish_date else '',
            'pdf_id': self.id,
            'pdf_url': self.pdf_url,
            'code_url': self.code_url,
            'category': self.category
        }
```

### 2.3 JSONæ•°æ®æ ¼å¼

**æ–‡ä»¶**: `docs/cv-arxiv-daily.json`

```json
{
  "Manipulation": {
    "paper_id_1": {
      "publish_date": "2025-12-08",
      "title": "Paper Title",
      "authors": "Author1, Author2",
      "pdf_url": "https://arxiv.org/abs/2512.xxxxx",
      "code_url": "https://github.com/xxx/xxx"
    }
  },
  "VLM": { ... },
  ...
}
```

---

## 3. APIæ¥å£è§„èŒƒ

### 3.1 RESTful APIè®¾è®¡

**åŸºç¡€URL**: `http://localhost:5001/api`

#### é€šç”¨å“åº”æ ¼å¼

**æˆåŠŸå“åº”**:
```json
{
  "success": true,
  "data": { ... },
  "message": "æ“ä½œæˆåŠŸ"
}
```

**é”™è¯¯å“åº”**:
```json
{
  "success": false,
  "error": "é”™è¯¯ä¿¡æ¯",
  "code": "ERROR_CODE"
}
```

### 3.2 APIç«¯ç‚¹è¯¦ç»†è¯´æ˜

#### 3.2.1 è·å–ç»Ÿè®¡ä¿¡æ¯

**ç«¯ç‚¹**: `GET /api/stats`

**æè¿°**: è·å–è®ºæ–‡æ•°é‡ç»Ÿè®¡ä¿¡æ¯

**è¯·æ±‚**: æ— å‚æ•°

**å“åº”**:
```json
{
  "success": true,
  "total": 4414,
  "stats": {
    "Manipulation": 1158,
    "VLM": 2174,
    "VLA": 504,
    "Humanoid": 288,
    "Dexterous": 214,
    "Locomotion": 23,
    "Planning": 19,
    "RL/IL": 17,
    "Perception": 17
  }
}
```

**å®ç°**:
```python
@app.route('/api/stats')
def get_stats():
    try:
        session = get_session()
        # æŸ¥è¯¢æ€»æ•°å’Œå„ç±»åˆ«æ•°é‡
        total = session.query(func.count(Paper.id)).scalar()
        stats = {}
        for category in CATEGORIES:
            count = session.query(func.count(Paper.id))\
                          .filter(Paper.category == category)\
                          .scalar()
            stats[category] = count or 0
        session.close()
        
        return jsonify({
            'success': True,
            'total': total or 0,
            'stats': stats
        })
    except Exception as e:
        return jsonify({'success': False, 'error': str(e)}), 500
```

---

#### 3.2.2 è·å–è®ºæ–‡åˆ—è¡¨

**ç«¯ç‚¹**: `GET /api/papers`

**æè¿°**: è·å–æ‰€æœ‰è®ºæ–‡æ•°æ®ï¼ŒæŒ‰ç±»åˆ«åˆ†ç»„

**è¯·æ±‚**: æ— å‚æ•°

**å“åº”**:
```json
{
  "success": true,
  "data": {
    "Manipulation": [
      {
        "id": "2512.05107",
        "title": "Paper Title",
        "authors": "Author Name",
        "date": "2025-12-08",
        "pdf_url": "https://arxiv.org/pdf/2512.05107",
        "code_url": "https://github.com/xxx/xxx",
        "category": "Manipulation"
      }
    ],
    "VLM": [ ... ]
  },
  "last_update": "2025-12-08 14:30:00",
  "total_count": 4414
}
```

**å®ç°**:
```python
@app.route('/api/papers')
def get_papers():
    try:
        session = get_session()
        papers_by_category = {}
        
        for category in CATEGORIES:
            papers = session.query(Paper)\
                           .filter(Paper.category == category)\
                           .order_by(Paper.publish_date.desc())\
                           .all()
            papers_by_category[category] = [p.to_dict() for p in papers]
        
        # è·å–æœ€åæ›´æ–°æ—¶é—´
        last_paper = session.query(Paper)\
                           .order_by(Paper.updated_at.desc())\
                           .first()
        last_update = last_paper.updated_at.strftime('%Y-%m-%d %H:%M:%S')\
                      if last_paper else None
        
        session.close()
        
        return jsonify({
            'success': True,
            'data': papers_by_category,
            'last_update': last_update,
            'total_count': sum(len(v) for v in papers_by_category.values())
        })
    except Exception as e:
        return jsonify({'success': False, 'error': str(e)}), 500
```

---

#### 3.2.3 æœç´¢è®ºæ–‡

**ç«¯ç‚¹**: `GET /api/search`

**æè¿°**: æ ¹æ®å…³é”®è¯å’Œç±»åˆ«æœç´¢è®ºæ–‡

**è¯·æ±‚å‚æ•°**:
| å‚æ•° | ç±»å‹ | å¿…å¡« | è¯´æ˜ |
|------|------|------|------|
| q | string | å¦ | æœç´¢å…³é”®è¯ï¼ˆæ ‡é¢˜æˆ–ä½œè€…ï¼‰ |
| category | string | å¦ | ç±»åˆ«ç­›é€‰ |

**è¯·æ±‚ç¤ºä¾‹**:
```
GET /api/search?q=robot&category=Manipulation
```

**å“åº”**:
```json
{
  "success": true,
  "data": {
    "papers": [
      {
        "id": "2512.05107",
        "title": "Robot Manipulation Paper",
        "authors": "Author Name",
        "date": "2025-12-08",
        "pdf_url": "https://arxiv.org/pdf/2512.05107",
        "code_url": "https://github.com/xxx/xxx",
        "category": "Manipulation"
      }
    ],
    "total": 1
  }
}
```

**å®ç°**:
```python
@app.route('/api/search')
def search_papers():
    try:
        query = request.args.get('q', '').strip()
        category = request.args.get('category', '').strip()
        
        session = get_session()
        papers_query = session.query(Paper)
        
        # å…³é”®è¯æœç´¢ï¼ˆæ ‡é¢˜æˆ–ä½œè€…ï¼‰
        if query:
            search_filter = or_(
                Paper.title.ilike(f'%{query}%'),
                Paper.authors.ilike(f'%{query}%')
            )
            papers_query = papers_query.filter(search_filter)
        
        # ç±»åˆ«ç­›é€‰
        if category:
            papers_query = papers_query.filter(Paper.category == category)
        
        # æŒ‰æ—¥æœŸå€’åº
        papers = papers_query.order_by(Paper.publish_date.desc()).all()
        
        session.close()
        
        return jsonify({
            'success': True,
            'data': {
                'papers': [p.to_dict() for p in papers],
                'total': len(papers)
            }
        })
    except Exception as e:
        return jsonify({'success': False, 'error': str(e)}), 500
```

---

#### 3.2.4 è§¦å‘è®ºæ–‡æŠ“å–

**ç«¯ç‚¹**: `POST /api/fetch`

**æè¿°**: æ‰‹åŠ¨è§¦å‘è®ºæ–‡æŠ“å–ä»»åŠ¡

**è¯·æ±‚Body**:
```json
{
  "max_results": 10,
  "config_path": "config.yaml"
}
```

**å‚æ•°è¯´æ˜**:
| å‚æ•° | ç±»å‹ | å¿…å¡« | é»˜è®¤å€¼ | è¯´æ˜ |
|------|------|------|--------|------|
| max_results | integer | å¦ | 20 | æ¯ä¸ªç±»åˆ«æŠ“å–æ•°é‡ |
| config_path | string | å¦ | "config.yaml" | é…ç½®æ–‡ä»¶è·¯å¾„ |

**å“åº”**:
```json
{
  "success": true,
  "message": "æŠ“å–ä»»åŠ¡å·²å¯åŠ¨"
}
```

**å®ç°**:
```python
@app.route('/api/fetch', methods=['POST'])
def trigger_fetch():
    global fetch_status
    
    # æ£€æŸ¥æ˜¯å¦å·²æœ‰ä»»åŠ¡è¿è¡Œ
    if fetch_status['running']:
        return jsonify({
            'success': False,
            'error': 'å·²æœ‰æŠ“å–ä»»åŠ¡æ­£åœ¨è¿è¡Œ'
        }), 400
    
    # è·å–å‚æ•°
    data = request.json or {}
    max_results = data.get('max_results', 20)
    config_path = data.get('config_path', 'config.yaml')
    
    def fetch_task():
        try:
            with fetch_status_lock:
                fetch_status['running'] = True
                fetch_status['message'] = 'æ­£åœ¨æŠ“å–...'
            
            # æ‰§è¡ŒæŠ“å–
            config = load_config(config_path)
            config['max_results'] = max_results
            demo(**config, fetch_status=fetch_status, 
                 fetch_status_lock=fetch_status_lock)
            
            with fetch_status_lock:
                fetch_status['message'] = 'æŠ“å–å®Œæˆï¼'
                fetch_status['last_update'] = datetime.now()\
                    .strftime('%Y-%m-%d %H:%M:%S')
        except Exception as e:
            logger.error(f"æŠ“å–ä»»åŠ¡å¤±è´¥: {e}")
            with fetch_status_lock:
                fetch_status['message'] = f'æŠ“å–å¤±è´¥: {str(e)}'
        finally:
            with fetch_status_lock:
                fetch_status['running'] = False
    
    # åå°çº¿ç¨‹æ‰§è¡Œ
    thread = threading.Thread(target=fetch_task)
    thread.daemon = True
    thread.start()
    
    return jsonify({
        'success': True,
        'message': 'æŠ“å–ä»»åŠ¡å·²å¯åŠ¨'
    })
```

---

#### 3.2.5 è·å–æŠ“å–çŠ¶æ€

**ç«¯ç‚¹**: `GET /api/fetch-status`

**æè¿°**: è·å–å½“å‰æŠ“å–ä»»åŠ¡çš„çŠ¶æ€

**è¯·æ±‚**: æ— å‚æ•°

**å“åº”**:
```json
{
  "running": false,
  "progress": 5,
  "total": 9,
  "current_keyword": "VLM",
  "message": "æŠ“å–å®Œæˆï¼",
  "last_update": "2025-12-08 14:30:00"
}
```

**å­—æ®µè¯´æ˜**:
| å­—æ®µ | ç±»å‹ | è¯´æ˜ |
|------|------|------|
| running | boolean | æ˜¯å¦æ­£åœ¨è¿è¡Œ |
| progress | integer | å½“å‰è¿›åº¦ï¼ˆå·²å®Œæˆç±»åˆ«æ•°ï¼‰ |
| total | integer | æ€»ç±»åˆ«æ•° |
| current_keyword | string | å½“å‰å¤„ç†çš„ç±»åˆ« |
| message | string | çŠ¶æ€æ¶ˆæ¯ |
| last_update | string | æœ€åæ›´æ–°æ—¶é—´ |

**å®ç°**:
```python
@app.route('/api/fetch-status')
def get_fetch_status():
    with fetch_status_lock:
        return jsonify(fetch_status)
```

---

## 4. æ ¸å¿ƒåŠŸèƒ½å®ç°

### 4.1 è®ºæ–‡æŠ“å–é€»è¾‘

#### 4.1.1 æŠ“å–æµç¨‹

```python
def get_daily_papers(topic, query="slam", max_results=2):
    """
    æŠ“å–è®ºæ–‡ä¸»å‡½æ•°
    
    Args:
        topic: ç ”ç©¶ç±»åˆ«
        query: æœç´¢æŸ¥è¯¢ï¼ˆORè¿æ¥çš„å…³é”®è¯ï¼‰
        max_results: æœ€å¤§æŠ“å–æ•°é‡
    
    Returns:
        content: è®ºæ–‡æ•°æ®å­—å…¸
        content_to_web: Webæ ¼å¼æ•°æ®å­—å…¸
    """
    
    # 1. æ£€æŸ¥å½“å‰ç±»åˆ«è®ºæ–‡æ•°é‡
    current_count = æŸ¥è¯¢æ•°æ®åº“ä¸­è¯¥ç±»åˆ«çš„è®ºæ–‡æ€»æ•°
    
    # 2. æ™ºèƒ½è°ƒæ•´æŠ“å–æ•°é‡
    if current_count < 1000:
        # è¡¥é½ç­–ç•¥ï¼šå¢åŠ æŠ“å–é‡ï¼Œä½†ä¸è¶…è¿‡3å€
        shortage = 1000 - current_count
        max_results = min(max_results + shortage, max_results * 3)
        logging.info(f"{topic}: å½“å‰{current_count}ç¯‡ï¼Œå¢åŠ åˆ°{max_results}ç¯‡")
    
    # 3. åˆ›å»ºArXivå®¢æˆ·ç«¯
    client = arxiv.Client(
        page_size=100,
        delay_seconds=3.0,  # å»¶è¿Ÿé¿å…é€Ÿç‡é™åˆ¶
        num_retries=3
    )
    
    # 4. æ„å»ºæœç´¢æŸ¥è¯¢ï¼ˆä¼˜å…ˆæœ€æ–°è®ºæ–‡ï¼‰
    search = arxiv.Search(
        query=query,
        max_results=max_results,
        sort_by=arxiv.SortCriterion.SubmittedDate,
        sort_order=arxiv.SortOrder.Descending  # å€’åºï¼šæœ€æ–°åœ¨å‰
    )
    
    # 5. æ‰§è¡ŒæŠ“å–
    results = client.results(search)
    for result in results:
        # æå–è®ºæ–‡ä¿¡æ¯
        paper_id = result.get_short_id()
        paper_title = result.title
        paper_authors = get_authors(result.authors)
        publish_time = result.published.date()
        pdf_url = arxiv_url + 'abs/' + paper_id
        
        # æå–ä»£ç é“¾æ¥ï¼ˆä»commentsï¼‰
        code_url = extract_code_url(result.comment)
        
        # ä¿å­˜åˆ°æ•°æ®åº“
        save_paper_to_db({
            'id': paper_id,
            'title': paper_title,
            'authors': paper_authors,
            'publish_date': publish_time,
            'pdf_url': pdf_url,
            'code_url': code_url,
            'category': topic
        }, topic)
    
    return content, content_to_web
```

#### 4.1.2 ä»£ç é“¾æ¥æå–

```python
def extract_code_url(comments):
    """
    ä»è®ºæ–‡commentsä¸­æå–ä»£ç é“¾æ¥
    
    Args:
        comments: è®ºæ–‡çš„commentså­—æ®µ
    
    Returns:
        code_url: ä»£ç é“¾æ¥ï¼ˆå¦‚æœæ‰¾åˆ°ï¼‰
    """
    if not comments:
        return None
    
    # æ­£åˆ™è¡¨è¾¾å¼åŒ¹é…HTTP/HTTPSé“¾æ¥
    urls = re.findall(r'(https?://[^\s,;]+)', comments)
    
    if urls:
        # ä¼˜å…ˆGitHubé“¾æ¥
        for url in urls:
            if 'github.com' in url.lower():
                return url
        # å¦åˆ™è¿”å›ç¬¬ä¸€ä¸ªé“¾æ¥
        return urls[0]
    
    return None
```

#### 4.1.3 æ™ºèƒ½å»é‡

```python
def is_duplicate_title(new_title, existing_titles, threshold=0.85):
    """
    æ£€æŸ¥æ ‡é¢˜æ˜¯å¦é‡å¤ï¼ˆåŸºäºç›¸ä¼¼åº¦ï¼‰
    
    Args:
        new_title: æ–°è®ºæ–‡æ ‡é¢˜
        existing_titles: å·²æœ‰è®ºæ–‡æ ‡é¢˜åˆ—è¡¨
        threshold: ç›¸ä¼¼åº¦é˜ˆå€¼ï¼ˆé»˜è®¤0.85ï¼‰
    
    Returns:
        is_dup: æ˜¯å¦é‡å¤
    """
    from difflib import SequenceMatcher
    
    # æ ‡å‡†åŒ–æ ‡é¢˜ï¼šè½¬å°å†™ã€ç§»é™¤æ ‡ç‚¹
    def normalize(title):
        return re.sub(r'[^\w\s]', '', title.lower())
    
    new_title_norm = normalize(new_title)
    
    for existing_title in existing_titles:
        existing_title_norm = normalize(existing_title)
        
        # è®¡ç®—ç›¸ä¼¼åº¦
        ratio = SequenceMatcher(None, new_title_norm, existing_title_norm).ratio()
        
        if ratio >= threshold:
            return True
    
    return False
```

### 4.2 å®šæ—¶ä»»åŠ¡å®ç°

#### 4.2.1 å®šæ—¶ä»»åŠ¡è°ƒåº¦å™¨

```python
def start_scheduler():
    """å¯åŠ¨å®šæ—¶ä»»åŠ¡è°ƒåº¦å™¨"""
    from apscheduler.schedulers.background import BackgroundScheduler
    from apscheduler.triggers.cron import CronTrigger
    
    scheduler = BackgroundScheduler()
    
    # 1. è®ºæ–‡æŠ“å–ä»»åŠ¡
    def scheduled_fetch():
        """å®šæ—¶æŠ“å–è®ºæ–‡ä»»åŠ¡ - ä½¿ç”¨fetch_new_data.fetch_papers()å‡½æ•°"""
        global fetch_status
        if fetch_status['running']:
            logger.info("å®šæ—¶æŠ“å–ä»»åŠ¡è·³è¿‡ï¼šå·²æœ‰ä»»åŠ¡æ­£åœ¨è¿è¡Œ")
            return
        
        try:
            logger.info("=" * 60)
            logger.info("å¼€å§‹æ‰§è¡Œå®šæ—¶è®ºæ–‡æŠ“å–ä»»åŠ¡...")
            logger.info("=" * 60)
            
            # ä½¿ç”¨ç»Ÿä¸€çš„å‡½æ•°è°ƒç”¨æ–¹å¼ï¼ˆä¸æ‰‹åŠ¨åˆ·æ–°ä¸€è‡´ï¼‰
            from fetch_new_data import fetch_papers
            fetch_papers()
            
            logger.info("=" * 60)
            logger.info("å®šæ—¶è®ºæ–‡æŠ“å–ä»»åŠ¡å®Œæˆ")
            logger.info("=" * 60)
        except Exception as e:
            logger.error("=" * 60)
            logger.error(f"å®šæ—¶è®ºæ–‡æŠ“å–ä»»åŠ¡å¤±è´¥: {e}")
            logger.error("=" * 60)
            import traceback
            logger.error(traceback.format_exc())
    
    # 2. æ–°é—»æŠ“å–ä»»åŠ¡
    def scheduled_fetch_news():
        """å®šæ—¶æŠ“å–æ–°é—»ä¿¡æ¯ä»»åŠ¡"""
        try:
            logger.info("=" * 60)
            logger.info("å¼€å§‹æ‰§è¡Œå®šæ—¶æ–°é—»ä¿¡æ¯æŠ“å–ä»»åŠ¡...")
            logger.info("=" * 60)
            from fetch_news import fetch_and_save_news
            fetch_and_save_news()
            logger.info("=" * 60)
            logger.info("å®šæ—¶æ–°é—»ä¿¡æ¯æŠ“å–ä»»åŠ¡å®Œæˆ")
            logger.info("=" * 60)
        except Exception as e:
            logger.error("=" * 60)
            logger.error(f"å®šæ—¶æ–°é—»ä¿¡æ¯æŠ“å–ä»»åŠ¡å¤±è´¥: {e}")
            logger.error("=" * 60)
            import traceback
            logger.error(traceback.format_exc())
    
    # 3. æ‹›è˜ä¿¡æ¯æŠ“å–ä»»åŠ¡
    def scheduled_fetch_jobs():
        """å®šæ—¶æŠ“å–æ‹›è˜ä¿¡æ¯ä»»åŠ¡"""
        try:
            logger.info("å¼€å§‹æ‰§è¡Œå®šæ—¶æ‹›è˜ä¿¡æ¯æŠ“å–ä»»åŠ¡...")
            from fetch_jobs import fetch_and_save_jobs
            fetch_and_save_jobs()
            logger.info("å®šæ—¶æ‹›è˜ä¿¡æ¯æŠ“å–ä»»åŠ¡å®Œæˆ")
        except Exception as e:
            logger.error(f"å®šæ—¶æ‹›è˜ä¿¡æ¯æŠ“å–ä»»åŠ¡å¤±è´¥: {e}")
    
    # é…ç½®è®ºæ–‡æŠ“å–å®šæ—¶ä»»åŠ¡ï¼ˆæ”¯æŒå¤šä¸ªcronè¡¨è¾¾å¼ï¼‰
    schedule_cron = os.getenv('AUTO_FETCH_SCHEDULE', '0 * * * *')
    if schedule_cron:
        cron_list = schedule_cron.split(';')
        for idx, cron_expr in enumerate(cron_list):
            cron_expr = cron_expr.strip()
            if not cron_expr:
                continue
            parts = cron_expr.split()
            if len(parts) == 5:
                minute, hour, day, month, weekday = parts
                scheduler.add_job(
                    scheduled_fetch,
                    trigger=CronTrigger(
                        minute=minute, hour=hour, day=day,
                        month=month, day_of_week=weekday
                    ),
                    id=f'hourly_fetch_papers_{idx}',
                    name=f'æ¯å°æ—¶è®ºæ–‡æŠ“å–_{idx+1}',
                    replace_existing=True
                )
    
    # é…ç½®æ–°é—»æŠ“å–å®šæ—¶ä»»åŠ¡
    news_schedule_cron = os.getenv('AUTO_FETCH_NEWS_SCHEDULE', '0 * * * *')
    if news_schedule_cron:
        cron_list = news_schedule_cron.split(';')
        for idx, cron_expr in enumerate(cron_list):
            cron_expr = cron_expr.strip()
            if not cron_expr:
                continue
            parts = cron_expr.split()
            if len(parts) == 5:
                minute, hour, day, month, weekday = parts
                scheduler.add_job(
                    scheduled_fetch_news,
                    trigger=CronTrigger(
                        minute=minute, hour=hour, day=day,
                        month=month, day_of_week=weekday
                    ),
                    id=f'hourly_fetch_news_{idx}',
                    name=f'æ–°é—»ä¿¡æ¯æŠ“å–_{idx+1}',
                    replace_existing=True
                )
    
    # é…ç½®æ‹›è˜ä¿¡æ¯æŠ“å–å®šæ—¶ä»»åŠ¡
    jobs_schedule_cron = os.getenv('AUTO_FETCH_JOBS_SCHEDULE', '0 * * * *')
    if jobs_schedule_cron:
        cron_list = jobs_schedule_cron.split(';')
        for idx, cron_expr in enumerate(cron_list):
            cron_expr = cron_expr.strip()
            if not cron_expr:
                continue
            parts = cron_expr.split()
            if len(parts) == 5:
                minute, hour, day, month, weekday = parts
                scheduler.add_job(
                    scheduled_fetch_jobs,
                    trigger=CronTrigger(
                        minute=minute, hour=hour, day=day,
                        month=month, day_of_week=weekday
                    ),
                    id=f'hourly_fetch_jobs_{idx}',
                    name=f'æ‹›è˜ä¿¡æ¯æŠ“å–_{idx+1}',
                    replace_existing=True
                )
    
    scheduler.start()
    return scheduler

def init_scheduler():
    """åˆå§‹åŒ–å®šæ—¶ä»»åŠ¡è°ƒåº¦å™¨ï¼ˆé€‚ç”¨äºGunicornç­‰ç”Ÿäº§ç¯å¢ƒï¼‰"""
    global scheduler
    if scheduler is not None:
        return scheduler
    
    auto_fetch_enabled = os.getenv('AUTO_FETCH_ENABLED', 'false').lower() == 'true'
    if auto_fetch_enabled:
        scheduler = start_scheduler()
        if scheduler:
            logger.info("âœ… å®šæ—¶ä»»åŠ¡è°ƒåº¦å™¨å·²å¯åŠ¨ï¼ˆé€‚ç”¨äºGunicornç­‰ç”Ÿäº§ç¯å¢ƒï¼‰")
        return scheduler
    else:
        logger.info("â„¹ï¸  è‡ªåŠ¨å®šæ—¶æŠ“å–æœªå¯ç”¨ï¼ˆè®¾ç½® AUTO_FETCH_ENABLED=true å¯ç”¨ï¼‰")
        return None
```

#### 4.2.2 Gunicornå¯åŠ¨é’©å­

**gunicorn_config.py**:
```python
def when_ready(server):
    """GunicornæœåŠ¡å™¨å°±ç»ªæ—¶è°ƒç”¨ï¼ˆæ‰€æœ‰workerå¯åŠ¨åï¼‰"""
    import logging
    logger = logging.getLogger('gunicorn.error')
    logger.info("=" * 60)
    logger.info("GunicornæœåŠ¡å™¨å·²å°±ç»ªï¼Œæ£€æŸ¥å®šæ—¶ä»»åŠ¡...")
    logger.info("=" * 60)
    
    # å¯¼å…¥appæ¨¡å—å¹¶å¯åŠ¨å®šæ—¶ä»»åŠ¡
    try:
        from app import init_scheduler
        scheduler = init_scheduler()
        if scheduler:
            logger.info("âœ… å®šæ—¶ä»»åŠ¡è°ƒåº¦å™¨å·²åœ¨Gunicornå¯åŠ¨æ—¶åˆå§‹åŒ–")
        else:
            logger.info("â„¹ï¸  å®šæ—¶ä»»åŠ¡æœªå¯ç”¨ï¼ˆAUTO_FETCH_ENABLED=falseï¼‰")
    except Exception as e:
        logger.error(f"âŒ å¯åŠ¨å®šæ—¶ä»»åŠ¡å¤±è´¥: {e}")
        import traceback
        logger.error(traceback.format_exc())

def on_exit(server):
    """GunicornæœåŠ¡å™¨é€€å‡ºæ—¶è°ƒç”¨"""
    import logging
    logger = logging.getLogger('gunicorn.error')
    try:
        from app import scheduler
        if scheduler:
            scheduler.shutdown()
            logger.info("å®šæ—¶ä»»åŠ¡è°ƒåº¦å™¨å·²å…³é—­")
    except:
        pass
```

### 4.3 æ•°æ®åº“æ“ä½œ

#### 4.3.1 ä¿å­˜è®ºæ–‡

```python
def save_paper_to_db(paper_data, category):
    """
    ä¿å­˜è®ºæ–‡åˆ°æ•°æ®åº“
    
    Args:
        paper_data: è®ºæ–‡æ•°æ®å­—å…¸
        category: ç±»åˆ«
    """
    from models import get_session, Paper
    
    session = get_session()
    
    try:
        # æ£€æŸ¥æ˜¯å¦å·²å­˜åœ¨
        existing = session.query(Paper).filter(Paper.id == paper_data['id']).first()
        
        if existing:
            # æ›´æ–°ç°æœ‰è®°å½•
            for key, value in paper_data.items():
                setattr(existing, key, value)
            existing.updated_at = datetime.now()
            logger.info(f"æ›´æ–°è®ºæ–‡: {paper_data['id']}")
        else:
            # åˆ›å»ºæ–°è®°å½•
            paper = Paper(**paper_data)
            session.add(paper)
            logger.info(f"æ–°å¢è®ºæ–‡: {paper_data['id']}")
        
        session.commit()
        return True
    except Exception as e:
        session.rollback()
        logger.error(f"ä¿å­˜è®ºæ–‡å¤±è´¥: {e}")
        return False
    finally:
        session.close()
```

#### 4.3.2 æŸ¥è¯¢ä¼˜åŒ–

**ä½¿ç”¨ç´¢å¼•**:
```python
# categoryç´¢å¼•
papers = session.query(Paper)\
                .filter(Paper.category == 'Manipulation')\
                .all()  # ä½¿ç”¨idx_categoryç´¢å¼•

# æ—¥æœŸç´¢å¼•
papers = session.query(Paper)\
                .filter(Paper.publish_date >= '2025-01-01')\
                .all()  # ä½¿ç”¨idx_publish_dateç´¢å¼•
```

**æ‰¹é‡æŸ¥è¯¢**:
```python
# ä¸€æ¬¡æ€§è·å–æ‰€æœ‰ç±»åˆ«
papers_by_category = {}
all_papers = session.query(Paper).all()

for paper in all_papers:
    if paper.category not in papers_by_category:
        papers_by_category[paper.category] = []
    papers_by_category[paper.category].append(paper)
```

---

## 5. å‰ç«¯å®ç°

### 5.1 æ ¸å¿ƒJavaScriptå‡½æ•°

#### 5.1.1 æ•°æ®åŠ è½½

```javascript
// åŠ è½½ç»Ÿè®¡ä¿¡æ¯
async function loadStats() {
    try {
        const response = await fetch('/api/stats');
        const result = await response.json();
        
        if (result.success) {
            renderStats(result.stats, result.total);
        }
    } catch (error) {
        console.error('åŠ è½½ç»Ÿè®¡å¤±è´¥:', error);
    }
}

// åŠ è½½è®ºæ–‡åˆ—è¡¨
async function loadPapers() {
    const container = document.getElementById('papersContainer');
    container.innerHTML = '<div class="loading-spinner">...</div>';
    
    try {
        const response = await fetch('/api/papers');
        const result = await response.json();
        
        if (result.success) {
            papersData = result.data;
            renderPapers(result.data);
            
            if (result.last_update) {
                updateLastUpdateTime(result.last_update);
            }
        }
    } catch (error) {
        console.error('åŠ è½½è®ºæ–‡å¤±è´¥:', error);
    }
}
```

#### 5.1.2 æœç´¢åŠŸèƒ½

```javascript
async function performSearch() {
    const query = document.getElementById('searchInput').value.trim();
    const category = document.getElementById('categoryFilter').value;
    
    if (!query && !category) {
        alert('è¯·è¾“å…¥æœç´¢å…³é”®è¯æˆ–é€‰æ‹©ç±»åˆ«');
        return;
    }
    
    const resultsDiv = document.getElementById('searchResults');
    const clearBtn = document.getElementById('clearSearchBtn');
    const papersContainer = document.getElementById('papersContainer');
    const tabs = document.getElementById('tabs');
    
    // éšè—åŸè®ºæ–‡åˆ—è¡¨
    papersContainer.style.display = 'none';
    tabs.style.display = 'none';
    
    // æ˜¾ç¤ºæœç´¢ç»“æœ
    resultsDiv.classList.remove('hidden');
    clearBtn.classList.remove('hidden');
    
    try {
        const params = new URLSearchParams();
        if (query) params.append('q', query);
        if (category) params.append('category', category);
        
        const response = await fetch(`/api/search?${params}`);
        const result = await response.json();
        
        if (result.success) {
            displaySearchResults(result.data.papers, result.data.total, query, category);
        }
    } catch (error) {
        console.error('æœç´¢å¤±è´¥:', error);
    }
}

// æ¸…é™¤æœç´¢
function clearSearch() {
    document.getElementById('searchInput').value = '';
    document.getElementById('categoryFilter').value = '';
    document.getElementById('searchResults').classList.add('hidden');
    document.getElementById('clearSearchBtn').classList.add('hidden');
    document.getElementById('papersContainer').style.display = 'block';
    document.getElementById('tabs').style.display = 'flex';
    
    // å¹³æ»‘æ»šåŠ¨
    document.querySelector('.papers-list-section').scrollIntoView({ 
        behavior: 'smooth',
        block: 'start'
    });
}
```

### 5.2 CSSè®¾è®¡è§„èŒƒ

#### 5.2.1 è®¾è®¡ç³»ç»Ÿ

```css
:root {
    /* é¢œè‰²ç³»ç»Ÿ */
    --primary-color: #2563eb;      /* ä¸»è‰²-è“è‰² */
    --secondary-color: #10b981;    /* è¾…è‰²-ç»¿è‰² */
    --danger-color: #ef4444;       /* å±é™©-çº¢è‰² */
    --bg-color: #f8fafc;           /* èƒŒæ™¯è‰² */
    --card-bg: #ffffff;            /* å¡ç‰‡èƒŒæ™¯ */
    --text-primary: #1e293b;       /* ä¸»æ–‡å­— */
    --text-secondary: #64748b;     /* æ¬¡æ–‡å­— */
    --border-color: #e2e8f0;       /* è¾¹æ¡†è‰² */
    
    /* é˜´å½±ç³»ç»Ÿ */
    --shadow: 0 1px 3px rgba(0,0,0,0.1);
    --shadow-lg: 0 10px 15px rgba(0,0,0,0.1);
    
    /* é—´è·ç³»ç»Ÿ */
    --spacing-xs: 4px;
    --spacing-sm: 8px;
    --spacing-md: 16px;
    --spacing-lg: 24px;
    --spacing-xl: 32px;
}
```

#### 5.2.2 å“åº”å¼è®¾è®¡

```css
/* æ¡Œé¢ç«¯ */
@media (min-width: 1024px) {
    .container {
        max-width: 1400px;
    }
    
    .stats-grid {
        grid-template-columns: repeat(5, 1fr);
    }
}

/* å¹³æ¿ */
@media (max-width: 1024px) and (min-width: 768px) {
    .container {
        max-width: 100%;
        padding: 15px;
    }
    
    .stats-grid {
        grid-template-columns: repeat(3, 1fr);
    }
}

/* ç§»åŠ¨ç«¯ */
@media (max-width: 768px) {
    .container {
        padding: 10px;
    }
    
    .stats-grid {
        grid-template-columns: repeat(2, 1fr);
    }
    
    .header-title h1 {
        font-size: 1.8rem;
    }
}
```

---

## 6. é…ç½®ç®¡ç†

### 6.1 é…ç½®æ–‡ä»¶ç»“æ„

**æ–‡ä»¶**: `config.yaml`

```yaml
# åŸºç¡€é…ç½®
max_results: 20                    # é»˜è®¤æ¯ç±»æŠ“å–æ•°é‡
publish_readme: true               # æ˜¯å¦å‘å¸ƒREADME
publish_gitpage: false             # æ˜¯å¦å‘å¸ƒGitHub Pages
publish_wechat: false              # æ˜¯å¦å‘å¸ƒå¾®ä¿¡æ ¼å¼
show_badge: false                  # æ˜¯å¦æ˜¾ç¤ºå¾½ç« 
update_paper_links: false          # æ˜¯å¦æ›´æ–°è®ºæ–‡é“¾æ¥

# æ–‡ä»¶è·¯å¾„
json_readme_path: './docs/cv-arxiv-daily.json'
json_gitpage_path: './docs/cv-arxiv-daily-web.json'
json_wechat_path: './docs/cv-arxiv-daily-wechat.json'
md_readme_path: './README.md'
md_gitpage_path: './docs/index.md'

# å…³é”®è¯é…ç½®
keywords:
  "Perception":
    filters:
      - "Robot Perception"
      - "Visual Perception"
      - "Sensor Fusion"
  
  "VLM":
    filters:
      - "Vision Language Model"
      - "Vision-Language Model"
      - "Multimodal Learning"
  
  "Planning":
    filters:
      - "Robot Planning"
      - "Motion Planning"
      - "Task Planning"
  
  "RL/IL":
    filters:
      - "Reinforcement Learning"
      - "Imitation Learning"
      - "Robot Learning"
  
  "Manipulation":
    filters:
      - "Robot Manipulation"
      - "Robotic Manipulation"
      - "Grasping"
  
  "Locomotion":
    filters:
      - "Locomotion"
      - "Mobile Robot"
      - "Navigation"
  
  "Dexterous":
    filters:
      - "Dexterous Manipulation"
      - "Dexterous Hand"
      - "Dexterity"
  
  "VLA":
    filters:
      - "Vision Language Action"
      - "Vision-Language-Action"
      - "Embodied Agent"
  
  "Humanoid":
    filters:
      - "Humanoid Robot"
      - "Humanoid"
      - "Bipedal Robot"
```

### 6.2 ç¯å¢ƒå˜é‡

```bash
# æ•°æ®åº“é…ç½®ï¼ˆPostgreSQLï¼‰
DATABASE_URL=postgresql://robotics_user:robotics_password@localhost:5432/robotics_arxiv

# å¯é€‰ï¼šç‹¬ç«‹æ•°æ®åº“é…ç½®
# JOBS_DATABASE_URL=postgresql://...
# NEWS_DATABASE_URL=postgresql://...
# DATASETS_DATABASE_URL=postgresql://...

# å‘åå…¼å®¹SQLiteï¼ˆå¯é€‰ï¼‰
# DATABASE_URL=sqlite:///./papers.db

# æœåŠ¡å™¨é…ç½®
PORT=5001
DEBUG=on                           # on/off

# è‡ªåŠ¨æŠ“å–é…ç½®
AUTO_FETCH_ENABLED=true            # true/false
AUTO_FETCH_SCHEDULE="0 * * * *"   # Cronè¡¨è¾¾å¼ï¼ˆè®ºæ–‡æŠ“å–ï¼Œé»˜è®¤æ¯å°æ—¶æ•´ç‚¹ï¼‰
AUTO_FETCH_NEWS_SCHEDULE="0 * * * *"  # Cronè¡¨è¾¾å¼ï¼ˆæ–°é—»æŠ“å–ï¼Œé»˜è®¤æ¯å°æ—¶æ•´ç‚¹ï¼‰
AUTO_FETCH_JOBS_SCHEDULE="0 * * * *"  # Cronè¡¨è¾¾å¼ï¼ˆæ‹›è˜æŠ“å–ï¼Œé»˜è®¤æ¯å°æ—¶æ•´ç‚¹ï¼‰
AUTO_FETCH_MAX_RESULTS=100         # æ¯æ¬¡æŠ“å–æ•°é‡ï¼ˆé»˜è®¤100ç¯‡ï¼‰
SEMANTIC_UPDATE_LIMIT=200         # Semantic Scholaræ¯æ¬¡æ›´æ–°æ•°é‡ï¼ˆé»˜è®¤200ç¯‡ï¼‰

# æ—¥å¿—é…ç½®
LOG_LEVEL=INFO                     # DEBUG/INFO/WARNING/ERROR
```

---

## 7. éƒ¨ç½²æ–¹æ¡ˆ

### 7.1 æœ¬åœ°å¼€å‘ç¯å¢ƒ

```bash
# 1. å…‹éš†é¡¹ç›®
git clone <repository_url>
cd robotics_arXiv_daily

# 2. åˆ›å»ºè™šæ‹Ÿç¯å¢ƒ
python3 -m venv venv
source venv/bin/activate

# 3. å®‰è£…ä¾èµ–
pip install -r requirements.txt

# 4. åˆå§‹åŒ–æ•°æ®åº“
python3 init_database.py

# 5. å¯åŠ¨æœåŠ¡å™¨
python3 app.py
```

### 7.2 ç”Ÿäº§ç¯å¢ƒéƒ¨ç½²

#### 7.2.1 ä½¿ç”¨Gunicorn

```bash
# å®‰è£…Gunicorn
pip install gunicorn

# å¯åŠ¨ï¼ˆ4ä¸ªworkerè¿›ç¨‹ï¼‰
gunicorn -w 4 -b 0.0.0.0:5001 app:app
```

#### 7.2.2 ä½¿ç”¨Nginxåå‘ä»£ç†

```nginx
server {
    listen 80;
    server_name your-domain.com;
    
    location / {
        proxy_pass http://127.0.0.1:5001;
        proxy_set_header Host $host;
        proxy_set_header X-Real-IP $remote_addr;
        proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for;
    }
    
    location /static/ {
        alias /path/to/robotics_arXiv_daily/static/;
        expires 30d;
    }
}
```

#### 7.2.3 ä½¿ç”¨systemdæœåŠ¡

**æ–‡ä»¶**: `/etc/systemd/system/robotics-arxiv.service`

```ini
[Unit]
Description=Robotics ArXiv Daily
After=network.target

[Service]
Type=simple
User=your-user
WorkingDirectory=/path/to/robotics_arXiv_daily
Environment="AUTO_FETCH_ENABLED=true"
Environment="AUTO_FETCH_SCHEDULE=0 2 * * *"
Environment="AUTO_FETCH_MAX_RESULTS=10"
ExecStart=/path/to/venv/bin/gunicorn -w 4 -b 0.0.0.0:5001 app:app
Restart=always

[Install]
WantedBy=multi-user.target
```

```bash
# å¯åŠ¨æœåŠ¡
sudo systemctl start robotics-arxiv
sudo systemctl enable robotics-arxiv

# æŸ¥çœ‹çŠ¶æ€
sudo systemctl status robotics-arxiv
```

### 7.3 Dockeréƒ¨ç½²

**Dockerfile**:
```dockerfile
FROM python:3.9-slim

WORKDIR /app

# å®‰è£…ä¾èµ–
COPY requirements.txt .
RUN pip install --no-cache-dir -r requirements.txt

# å¤åˆ¶ä»£ç 
COPY . .

# åˆå§‹åŒ–æ•°æ®åº“
RUN python3 init_database.py

# æš´éœ²ç«¯å£
EXPOSE 5001

# å¯åŠ¨åº”ç”¨ï¼ˆä½¿ç”¨é…ç½®æ–‡ä»¶ï¼Œè‡ªåŠ¨å¯åŠ¨å®šæ—¶ä»»åŠ¡ï¼‰
CMD ["gunicorn", "-c", "gunicorn_config.py", "app:app"]
```

**docker-compose.yml**:
```yaml
version: '3.8'

services:
  web:
    build: .
    ports:
      - "5001:5001"
    volumes:
      - ./papers.db:/app/papers.db
      - ./docs:/app/docs
    environment:
      - AUTO_FETCH_ENABLED=true
      - AUTO_FETCH_SCHEDULE=0 * * * *  # æ¯å°æ—¶æ•´ç‚¹æ‰§è¡Œè®ºæ–‡æŠ“å–
      - AUTO_FETCH_NEWS_SCHEDULE=0 * * * *  # æ¯å°æ—¶æ•´ç‚¹æ‰§è¡Œæ–°é—»æŠ“å–
      - AUTO_FETCH_JOBS_SCHEDULE=0 * * * *  # æ¯å°æ—¶æ•´ç‚¹æ‰§è¡Œæ‹›è˜æŠ“å–
      - AUTO_FETCH_MAX_RESULTS=100
    restart: always
```

---

## 8. ç›‘æ§ä¸ç»´æŠ¤

### 8.1 æ—¥å¿—ç®¡ç†

**æ—¥å¿—é…ç½®**:
```python
import logging

logging.basicConfig(
    level=logging.INFO,
    format='[%(asctime)s %(levelname)s] %(message)s',
    datefmt='%m/%d/%Y %H:%M:%S',
    handlers=[
        logging.FileHandler('app.log'),      # æ–‡ä»¶æ—¥å¿—
        logging.StreamHandler()              # æ§åˆ¶å°æ—¥å¿—
    ]
)
```

**æ—¥å¿—çº§åˆ«**:
- DEBUG: è¯¦ç»†è°ƒè¯•ä¿¡æ¯
- INFO: å¸¸è§„æ“ä½œä¿¡æ¯
- WARNING: è­¦å‘Šä¿¡æ¯
- ERROR: é”™è¯¯ä¿¡æ¯

### 8.2 æ€§èƒ½ç›‘æ§

**å…³é”®æŒ‡æ ‡**:
- APIå“åº”æ—¶é—´
- æ•°æ®åº“æŸ¥è¯¢æ—¶é—´
- æŠ“å–æˆåŠŸç‡
- ç³»ç»Ÿèµ„æºä½¿ç”¨ç‡ï¼ˆCPUã€å†…å­˜ã€ç£ç›˜ï¼‰

**ç›‘æ§å·¥å…·**:
- Flaskå†…ç½®æ€§èƒ½åˆ†æ
- SQLAlchemyæŸ¥è¯¢æ—¥å¿—
- ç³»ç»Ÿç›‘æ§ï¼ˆhtopã€iotopç­‰ï¼‰

### 8.3 æ•°æ®å¤‡ä»½

```bash
#!/bin/bash
# backup.sh - æ•°æ®åº“å¤‡ä»½è„šæœ¬

DATE=$(date +%Y%m%d_%H%M%S)
BACKUP_DIR="/path/to/backups"

# å¤‡ä»½SQLiteæ•°æ®åº“
cp papers.db "$BACKUP_DIR/papers_$DATE.db"

# å¤‡ä»½JSONæ–‡ä»¶
cp docs/cv-arxiv-daily.json "$BACKUP_DIR/papers_$DATE.json"

# æ¸…ç†30å¤©å‰çš„å¤‡ä»½
find "$BACKUP_DIR" -name "papers_*.db" -mtime +30 -delete
find "$BACKUP_DIR" -name "papers_*.json" -mtime +30 -delete

echo "å¤‡ä»½å®Œæˆ: $DATE"
```

**å®šæ—¶å¤‡ä»½** (crontab):
```bash
# æ¯å¤©å‡Œæ™¨3ç‚¹å¤‡ä»½
0 3 * * * /path/to/backup.sh >> /var/log/backup.log 2>&1
```

---

## 9. æµ‹è¯•

### 9.1 å•å…ƒæµ‹è¯•

```python
import unittest
from models import Paper
from save_paper_to_db import save_paper_to_db

class TestPaperModel(unittest.TestCase):
    def test_paper_creation(self):
        paper = Paper(
            id='2512.05107',
            title='Test Paper',
            category='Manipulation'
        )
        self.assertEqual(paper.id, '2512.05107')
    
    def test_paper_to_dict(self):
        paper = Paper(
            id='2512.05107',
            title='Test Paper',
            category='Manipulation'
        )
        data = paper.to_dict()
        self.assertIn('id', data)
        self.assertIn('title', data)

if __name__ == '__main__':
    unittest.main()
```

### 9.2 APIæµ‹è¯•

```bash
# æµ‹è¯•ç»Ÿè®¡æ¥å£
curl http://localhost:5001/api/stats

# æµ‹è¯•è®ºæ–‡åˆ—è¡¨æ¥å£
curl http://localhost:5001/api/papers

# æµ‹è¯•æœç´¢æ¥å£
curl "http://localhost:5001/api/search?q=robot&category=Manipulation"

# æµ‹è¯•æŠ“å–æ¥å£
curl -X POST http://localhost:5001/api/fetch \
  -H "Content-Type: application/json" \
  -d '{"max_results": 5}'

# æµ‹è¯•æŠ“å–çŠ¶æ€æ¥å£
curl http://localhost:5001/api/fetch-status
```

### 9.3 æ€§èƒ½æµ‹è¯•

```bash
# ä½¿ç”¨Apache Benchæµ‹è¯•
ab -n 100 -c 10 http://localhost:5001/api/stats

# è¾“å‡ºç¤ºä¾‹ï¼š
# Requests per second: 250.00 [#/sec]
# Time per request: 40.000 [ms]
```

---

## 10. æ•…éšœæ’æŸ¥

### 10.1 å¸¸è§é—®é¢˜

**é—®é¢˜1: ç«¯å£è¢«å ç”¨**
```bash
# æŸ¥æ‰¾å ç”¨ç«¯å£çš„è¿›ç¨‹
lsof -i:5001

# ç»“æŸè¿›ç¨‹
kill <PID>
```

**é—®é¢˜2: æ•°æ®åº“é”å®š**ï¼ˆå·²è§£å†³ï¼‰
```bash
# v1.3.0å·²å‡çº§åˆ°PostgreSQLï¼Œæ”¯æŒé«˜å¹¶å‘
# å¦‚æœä½¿ç”¨SQLiteé‡åˆ°é”å®šé—®é¢˜ï¼š
# 1. å…³é—­æ‰€æœ‰æ•°æ®åº“è¿æ¥ï¼Œæˆ–é‡å¯æœåŠ¡
# 2. å»ºè®®å‡çº§åˆ°PostgreSQLï¼šä½¿ç”¨ migrate_sqlite_to_postgresql.py
```

**é—®é¢˜3: ArXiv APIé™åˆ¶**
```
HTTP 429 Too Many Requests
# è§£å†³ï¼šå¢åŠ delay_secondsï¼Œå‡å°‘max_results
```

### 10.2 è°ƒè¯•æŠ€å·§

**å¯ç”¨DEBUGæ¨¡å¼**:
```python
app.run(debug=True, port=5001)
```

**æŸ¥çœ‹SQLæŸ¥è¯¢**:
```python
# å¯ç”¨SQLAlchemy echo
engine = create_engine(DATABASE_URL, echo=True)
```

**æŸ¥çœ‹APIè¯·æ±‚**:
```javascript
// æµè§ˆå™¨å¼€å‘è€…å·¥å…· -> Network
// æŸ¥çœ‹è¯·æ±‚å’Œå“åº”è¯¦æƒ…
```

---

## 11. é™„å½•

### 11.1 ä¾èµ–æ¸…å•

**requirements.txt**:
```
Flask==3.0.0
SQLAlchemy==2.0.23
arxiv==2.1.0
PyYAML==6.0.1
APScheduler==3.10.4
requests==2.31.0
```

### 11.2 Gitè§„èŒƒ

**åˆ†æ”¯ç­–ç•¥**:
- main: ç”Ÿäº§ç¯å¢ƒä»£ç 
- develop: å¼€å‘ç¯å¢ƒä»£ç 
- feature/*: åŠŸèƒ½å¼€å‘åˆ†æ”¯
- hotfix/*: ç´§æ€¥ä¿®å¤åˆ†æ”¯

**æäº¤è§„èŒƒ**:
```
feat: æ–°åŠŸèƒ½
fix: Bugä¿®å¤
docs: æ–‡æ¡£æ›´æ–°
style: ä»£ç æ ¼å¼è°ƒæ•´
refactor: ä»£ç é‡æ„
test: æµ‹è¯•ç›¸å…³
chore: æ„å»º/å·¥å…·é“¾ç›¸å…³
```

### 11.3 å‚è€ƒèµ„æº

- [Flaskæ–‡æ¡£](https://flask.palletsprojects.com/)
- [SQLAlchemyæ–‡æ¡£](https://docs.sqlalchemy.org/)
- [PostgreSQLæ–‡æ¡£](https://www.postgresql.org/docs/)
- [ArXiv APIæ–‡æ¡£](https://info.arxiv.org/help/api/index.html)
- [APScheduleræ–‡æ¡£](https://apscheduler.readthedocs.io/)

### 11.4 å˜æ›´è®°å½•

| ç‰ˆæœ¬ | æ—¥æœŸ | å˜æ›´å†…å®¹ | è´Ÿè´£äºº |
|------|------|---------|--------|
| v1.0 | 2025-12-08 | åˆå§‹ç‰ˆæœ¬ | AIåŠ©æ‰‹ |
| v1.1 | 2025-12-08 | æ›´æ–°æŠ“å–ç­–ç•¥å’Œæœç´¢ä¼˜åŒ– | AIåŠ©æ‰‹ |
| v1.2 | 2025-12-09 | æ·»åŠ æ–°é—»ã€æ‹›è˜ã€æ•°æ®é›†åŠŸèƒ½æŠ€æœ¯å®ç° | AIåŠ©æ‰‹ |
| v1.2.2 | 2025-12-10 | æ›´æ–°åˆ·æ–°åŠŸèƒ½å®ç°ã€æ–°è®ºæ–‡çº¢ç‚¹æç¤º | AIåŠ©æ‰‹ |
| v1.2.3 | 2025-12-11 | æ›´æ–°å®šæ—¶ä»»åŠ¡æœºåˆ¶ï¼ˆGunicornæ”¯æŒï¼‰ã€åˆ·æ–°åŠŸèƒ½ä¼˜åŒ– | AIåŠ©æ‰‹ |
| v1.3.0 | 2025-12-11 | PostgreSQLæ•°æ®åº“å…¨é¢å‡çº§ã€å…·èº«èµ›åšç¥ç¦è¯­ç³»ç»Ÿä¼˜åŒ– | AIåŠ©æ‰‹ |
| v1.3.1 | 2025-12-13 | PRDå’ŒSPECæ–‡æ¡£ä¼˜åŒ–ã€é¡¹ç›®ä»»åŠ¡æ¸…å•åˆ›å»º | AIåŠ©æ‰‹ |

---

**æ–‡æ¡£ç»´æŠ¤**: ä»£ç å˜æ›´ååŒæ­¥æ›´æ–°ï¼Œä¿æŒå®æ—¶æ›´æ–°  
**å®¡æ ¸çŠ¶æ€**: âœ… å·²å®¡æ ¸  
**ä¸‹æ¬¡è¯„å®¡**: æ¶æ„é‡å¤§å˜æ›´æ—¶  
**æœ€åæ›´æ–°**: 2025-12-19 - æ›´æ–°ç”Ÿäº§ç¯å¢ƒä¿¡æ¯ã€Bç«™æ•°æ®è‡ªåŠ¨æ›´æ–°æœºåˆ¶ã€ç®¡ç†åå°åŠŸèƒ½

---

## 13. ç”Ÿäº§ç¯å¢ƒéƒ¨ç½²ä¿¡æ¯

### 13.1 æœåŠ¡å™¨é…ç½®

**æœåŠ¡å™¨è·¯å¾„**ï¼š`/srv/EmbodiedPulse2026`  
**æœåŠ¡ç®¡ç†**ï¼šsystemd (`embodiedpulse.service`)  
**åº”ç”¨ç«¯å£**ï¼š`5001`  
**åå‘ä»£ç†**ï¼šNginx  
**æ•°æ®åº“**ï¼šPostgreSQL 15+

### 13.2 åŸŸåé…ç½®

**ç”¨æˆ·ç«¯åŸŸå**ï¼š
- `essay.gradmotion.com` - ä¸»ç«™ï¼ˆè®ºæ–‡é¡µé¢ï¼‰
- `login.gradmotion.com` - ç™»å½•é¡µé¢

**ç®¡ç†ç«¯åŸŸå**ï¼š
- `admin123.gradmotion.com` - ç®¡ç†åå°

**SSLè¯ä¹¦**ï¼šLet's Encryptï¼ˆè‡ªåŠ¨ç»­æœŸï¼‰

### 13.3 æ•°æ®åº“é…ç½®

**PostgreSQLæ•°æ®åº“**ï¼š
- æ•°æ®åº“åï¼š`embodied_pulse`
- è¿æ¥æ± ï¼š`pool_size=10, max_overflow=20`
- è‡ªåŠ¨é‡è¿ï¼š`pool_pre_ping=True`

**æ•°æ®åº“åˆ—è¡¨**ï¼š
- `papers` - è®ºæ–‡æ•°æ®
- `bilibili` - Bç«™æ•°æ®ï¼ˆUPä¸»å’Œè§†é¢‘ï¼‰
- `jobs` - æ‹›è˜ä¿¡æ¯
- `news` - æ–°é—»æ•°æ®
- `datasets` - æ•°æ®é›†æ•°æ®
- `users` - ç”¨æˆ·æ•°æ®

### 13.4 å®šæ—¶ä»»åŠ¡é…ç½®

**ç¯å¢ƒå˜é‡**ï¼š
- `AUTO_FETCH_ENABLED=true` - å¯ç”¨å®šæ—¶ä»»åŠ¡
- `AUTO_FETCH_SCHEDULE=0 * * * *` - è®ºæ–‡æŠ“å–ï¼ˆæ¯å°æ—¶ï¼‰
- `AUTO_FETCH_BILIBILI_SCHEDULE=0 */6 * * *` - Bç«™æ•°æ®ï¼ˆæ¯6å°æ—¶ï¼‰
- `AUTO_UPDATE_VIDEO_PLAYS_SCHEDULE=0 2 * * *` - æ’­æ”¾é‡æ›´æ–°ï¼ˆæ¯å¤©2ç‚¹ï¼‰

### 13.5 æœ€æ–°æŠ€æœ¯æ›´æ–°ï¼ˆv2.0ï¼‰

**Bç«™æ•°æ®è‡ªåŠ¨æ›´æ–°**ï¼š
- âœ… æ’­æ”¾é‡æ›´æ–°é€»è¾‘ä¿®å¤
- âœ… ç®¡ç†åå°æ‰‹åŠ¨æ›´æ–°åŠŸèƒ½
- âœ… è‡ªåŠ¨æ›´æ–°æœºåˆ¶å®Œå–„
- âœ… æ•°æ®å®Œæ•´æ€§æ£€æŸ¥è„šæœ¬

**æ–‡æ¡£æ•´ç†**ï¼š
- âœ… æ–‡æ¡£åˆå¹¶å’Œå½’ç±»æ•´ç†
- âœ… æœåŠ¡å™¨è„šæœ¬å‘½ä»¤æ‰‹å†Œ
- âœ… è´¦å·æ‰‹å†Œåˆ›å»º

---

## 12. æœ€æ–°æŠ€æœ¯æ›´æ–°

### 12.1 æ•°æ®åº“å‡çº§ï¼ˆv1.3.0ï¼‰

**ä»SQLiteå‡çº§åˆ°PostgreSQL**ï¼š
- âœ… æ‰€æœ‰æ•°æ®åº“ï¼ˆpapers, jobs, news, datasetsï¼‰å·²è¿ç§»åˆ°PostgreSQL
- âœ… æ”¯æŒè¿æ¥æ± ï¼ˆpool_size=10, max_overflow=20ï¼‰
- âœ… è‡ªåŠ¨é‡è¿æœºåˆ¶ï¼ˆpool_pre_ping=Trueï¼‰
- âœ… å‘åå…¼å®¹SQLiteï¼ˆé€šè¿‡ç¯å¢ƒå˜é‡åˆ‡æ¢ï¼‰

### 12.2 å®šæ—¶ä»»åŠ¡ä¼˜åŒ–ï¼ˆv1.2.3ï¼‰

**Gunicornæ”¯æŒ**ï¼š
- âœ… é€šè¿‡`when_ready` hookè‡ªåŠ¨å¯åŠ¨å®šæ—¶ä»»åŠ¡
- âœ… æ”¯æŒå¼€å‘ç¯å¢ƒå’Œç”Ÿäº§ç¯å¢ƒè‡ªåŠ¨å¯åŠ¨
- âœ… å¤šä»»åŠ¡æ”¯æŒï¼ˆè®ºæ–‡ã€æ–°é—»ã€æ‹›è˜ã€Semantic Scholarï¼‰

### 12.3 åŠŸèƒ½å¢å¼º

**æ–°å¢åŠŸèƒ½**ï¼š
- âœ… å…·èº«24hæ–°é—»ğŸ”¥ï¼ˆå¤šæºæ–°é—»èšåˆï¼‰
- âœ… æ‹›è˜ä¿¡æ¯å±•ç¤ºï¼ˆGitHub Jobs APIï¼‰
- âœ… æ•°æ®é›†ä¿¡æ¯å±•ç¤ºï¼ˆJuejinæ•°æ®é›†ï¼‰
- âœ… Semantic Scholaræ•°æ®é›†æˆ
- âœ… å…·èº«èµ›åšç¥ç¦è¯­ç³»ç»Ÿï¼ˆ100æ¡/æ–¹å‘ï¼‰

### 12.4 éƒ¨ç½²ä¼˜åŒ–

**Dockeréƒ¨ç½²**ï¼š
- âœ… docker-entrypoint.shå¯åŠ¨è„šæœ¬
- âœ… æ•°æ®åº“åˆå§‹åŒ–åœ¨å®¹å™¨å¯åŠ¨æ—¶æ‰§è¡Œ
- âœ… å¥åº·æ£€æŸ¥ä¼˜åŒ–
- âœ… GitHub Actionsè‡ªåŠ¨éƒ¨ç½²

