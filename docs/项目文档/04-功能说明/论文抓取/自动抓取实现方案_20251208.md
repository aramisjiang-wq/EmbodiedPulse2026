# 自动抓取论文数据 - 实现说明

## 📋 当前实现状态

### ✅ 已实现：手动触发抓取
- **Web 界面触发**: 点击"抓取新论文"按钮
- **API 触发**: `POST /api/fetch`
- **后台执行**: 使用线程异步执行，不阻塞 Web 服务

### ❌ 未实现：自动定时抓取
- 目前**没有**自动定时抓取功能
- 需要手动触发或通过 API 调用

## 🔄 当前实现过程

### 1. 手动触发流程

```
用户点击按钮
    ↓
前端发送 POST /api/fetch
    ↓
后端接收请求
    ↓
检查是否已有任务运行
    ↓
创建后台线程执行抓取
    ↓
返回"任务已启动"
    ↓
后台线程执行：
  - 调用 ArXiv API
  - 解析论文数据
  - 保存到数据库
  - 更新 JSON 文件（备份）
    ↓
前端轮询 /api/fetch-status
    ↓
显示进度和结果
```

### 2. 代码实现位置

#### 后端 API (`app.py`)
```python
@app.route('/api/fetch', methods=['POST'])
def trigger_fetch():
    """触发论文抓取"""
    # 1. 检查任务状态
    # 2. 创建后台线程
    # 3. 执行抓取任务
    # 4. 返回响应
```

#### 抓取逻辑 (`daily_arxiv.py`)
```python
def demo(**config):
    """主抓取函数"""
    # 1. 遍历配置的关键词
    # 2. 调用 get_daily_papers() 获取论文
    # 3. 保存到数据库和 JSON
```

#### 数据保存 (`save_paper_to_db.py`)
```python
def save_paper_to_db(paper_data, category):
    """保存论文到数据库"""
    # 1. 检查是否已存在（去重）
    # 2. 创建或更新记录
    # 3. 提交到数据库
```

## 🚀 添加自动定时抓取

### 方案 1: 使用 APScheduler（推荐）

#### 安装依赖
```bash
pip install apscheduler
```

#### 实现代码
在 `app.py` 中添加：

```python
from apscheduler.schedulers.background import BackgroundScheduler
from apscheduler.triggers.cron import CronTrigger

# 创建调度器
scheduler = BackgroundScheduler()

def scheduled_fetch():
    """定时抓取任务"""
    try:
        config = load_config('config.yaml')
        config['max_results'] = 10  # 定时任务使用较小的数量
        config['update_paper_links'] = False
        demo(**config)
        logger.info("定时抓取任务完成")
    except Exception as e:
        logger.error(f"定时抓取任务失败: {e}")

# 每天凌晨2点执行
scheduler.add_job(
    scheduled_fetch,
    trigger=CronTrigger(hour=2, minute=0),
    id='daily_fetch',
    name='每日论文抓取',
    replace_existing=True
)

# 启动调度器
scheduler.start()
```

### 方案 2: 使用系统 Cron（Linux/Mac）

创建脚本 `auto_fetch.sh`:
```bash
#!/bin/bash
cd /path/to/robotics_arXiv_daily
source venv/bin/activate
python3 -c "from app import trigger_fetch; import requests; requests.post('http://localhost:5001/api/fetch', json={'max_results': 10})"
```

添加到 crontab:
```bash
# 每天凌晨2点执行
0 2 * * * /path/to/auto_fetch.sh
```

### 方案 3: 使用 GitHub Actions（云端）

如果部署在服务器上，可以使用 GitHub Actions 定时触发。

## 📊 实现流程图

```
┌─────────────────────────────────────┐
│     自动定时抓取（新增功能）          │
┌─────────────────────────────────────┐
│  APScheduler 定时器                  │
│  - 每天凌晨2点触发                   │
│  - 或每12小时触发                    │
└──────────────┬──────────────────────┘
               │
               ▼
┌─────────────────────────────────────┐
│  执行抓取任务                        │
│  1. 加载配置 (config.yaml)            │
│  2. 遍历关键词类别                   │
│  3. 调用 ArXiv API                   │
└──────────────┬──────────────────────┘
               │
               ▼
┌─────────────────────────────────────┐
│  数据处理和保存                      │
│  1. 解析论文数据                     │
│  2. 去重检查（数据库主键）            │
│  3. 保存到数据库                     │
│  4. 更新 JSON 文件（备份）            │
└──────────────┬──────────────────────┘
               │
               ▼
┌─────────────────────────────────────┐
│  完成通知（可选）                     │
│  - 日志记录                          │
│  - 邮件通知（可选）                  │
│  - Webhook 通知（可选）               │
└─────────────────────────────────────┘
```

## 🔧 配置选项

### 定时任务配置

可以在 `config.yaml` 中添加：

```yaml
# 自动抓取配置
auto_fetch:
  enabled: true
  schedule: "0 2 * * *"  # 每天凌晨2点
  max_results: 10         # 每次抓取数量
  categories:             # 指定抓取的类别
    - Manipulation
    - VLM
    - VLA
```

## ⚠️ 注意事项

1. **ArXiv API 速率限制**
   - 建议定时任务使用较小的 `max_results`（5-10）
   - 避免短时间内大量请求

2. **错误处理**
   - 定时任务应该有完善的错误处理
   - 记录日志以便排查问题

3. **资源占用**
   - 定时任务在后台运行，不影响 Web 服务
   - 注意内存和 CPU 使用

4. **数据去重**
   - 数据库主键确保不会重复插入
   - 已存在的论文会更新而不是重复

## 📝 总结

**当前状态**:
- ✅ 支持手动触发抓取（Web 界面 + API）
- ❌ 不支持自动定时抓取

**建议**:
- 添加 APScheduler 实现自动定时抓取
- 配置每天凌晨执行（避开高峰期）
- 使用较小的抓取数量避免速率限制

