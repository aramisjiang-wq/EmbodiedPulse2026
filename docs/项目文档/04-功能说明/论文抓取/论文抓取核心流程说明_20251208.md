# 论文抓取逻辑说明

## 📋 概述

当前系统通过 ArXiv API 自动抓取具身智能领域的论文，并保存到数据库和JSON文件中。

## 🔍 抓取逻辑

### 1. 数据源
- **API**: ArXiv API (http://arxiv.org/)
- **客户端**: 使用 `arxiv` Python 库的 `Client` API
- **配置**: 通过 `config.yaml` 文件配置搜索关键词

### 2. 抓取流程

```
1. 读取 config.yaml 配置文件
   ↓
2. 解析关键词配置，生成搜索查询字符串
   ↓
3. 对每个类别（Manipulation, VLM, VLA, Humanoid, Dexterous）：
   ├─ 构建 ArXiv 搜索查询
   ├─ 调用 ArXiv API 搜索论文
   ├─ 按提交日期排序（最新优先）
   ├─ 限制每类最多抓取 max_results 篇（默认20篇）
   └─ 解析论文信息
   ↓
4. 保存论文数据：
   ├─ 保存到 SQLite 数据库（papers.db）
   └─ 保存到 JSON 文件（docs/cv-arxiv-daily.json）
```

### 3. 搜索查询构建

对于每个类别，系统会将多个关键词用 `OR` 连接：

**示例 - Manipulation 类别**:
```
查询字符串: "Robot Manipulation" OR "Robotic Manipulation" OR "Robot Learning" OR "Imitation Learning"
```

### 4. 论文信息提取

每篇论文提取以下信息：
- **论文ID**: ArXiv ID（如：2504.13120）
- **标题**: Paper Title
- **作者**: Authors（完整列表）
- **摘要**: Abstract
- **PDF链接**: ArXiv PDF URL
- **代码链接**: 从论文的 comments 字段中提取（如果有）
- **发布日期**: Published Date
- **更新日期**: Updated Date
- **主要类别**: Primary Category

### 5. 速率限制处理

- **延迟设置**: `delay_seconds=3.0`（每次请求间隔3秒）
- **重试机制**: `num_retries=3`（失败重试3次）
- **额外延迟**: 每篇论文处理间隔 `time.sleep(1.0)` 秒
- **错误处理**: 捕获 `arxiv.HTTPError`（如429速率限制），继续处理已获取的数据

## 📚 抓取的论文类型

根据 `config.yaml` 配置，当前抓取以下5类论文：

### 1. Manipulation（操作控制）
**搜索关键词**:
- "Robot Manipulation"
- "Robotic Manipulation"
- "Robot Learning"
- "Imitation Learning"

**研究方向**: 机器人操作、抓取、模仿学习

### 2. VLM（视觉语言模型）
**搜索关键词**:
- "Vision Language Model"
- "Vision Language Models"
- "Vision-Language Model"
- "Vision-Language Models"

**研究方向**: 视觉理解、多模态学习、视觉推理

### 3. VLA（视觉语言动作）
**搜索关键词**:
- "Vision Language Action"
- "Vision-Language-Action"

**研究方向**: 端到端控制、指令跟随、具身智能体

### 4. Humanoid（人形机器人）
**搜索关键词**:
- "Humanoid Robot"
- "Humanoid"

**研究方向**: 双足行走、人形控制、全身协调

### 5. Dexterous（灵巧操作）
**搜索关键词**:
- "Dexterous Manipulation"
- "Dexterous hand"
- "Dexterity"
- "Dexterous"

**研究方向**: 精细操作、触觉感知、复杂任务

## ⚙️ 配置参数

### config.yaml 主要参数

```yaml
max_results: 20  # 每类论文最多抓取数量

keywords:
  "Manipulation":
    filters: ["Robot Manipulation", ...]
  "VLM":
    filters: ["Vision Language Model", ...]
  # ... 其他类别
```

### API 抓取参数

- **page_size**: 100（每页返回论文数）
- **delay_seconds**: 3.0（请求间隔）
- **num_retries**: 3（重试次数）
- **sort_by**: SubmittedDate（按提交日期排序）

## 🔄 抓取方式

### 1. 手动抓取（通过 Web UI）
- 访问 `/api/fetch` 端点
- 可配置 `max_results` 参数
- 在后台线程中执行，不阻塞主进程

### 2. 自动定时抓取（可选）
- 使用 APScheduler 定时任务
- 默认每天凌晨2点执行（UTC时间）
- 可通过环境变量配置：
  - `AUTO_FETCH_ENABLED`: 是否启用自动抓取
  - `AUTO_FETCH_SCHEDULE`: Cron 表达式（如：`0 2 * * *`）
  - `AUTO_FETCH_MAX_RESULTS`: 定时任务抓取数量（默认10篇）

## 💾 数据存储

### 1. 数据库存储（SQLite）
- **表名**: `papers`
- **字段**:
  - `id`: ArXiv ID（主键）
  - `title`: 论文标题
  - `authors`: 作者列表
  - `category`: 类别（Manipulation, VLM, VLA, Humanoid, Dexterous）
  - `publish_date`: 发布日期
  - `updated_at`: 更新时间
  - `pdf_url`: PDF链接
  - `code_url`: 代码链接（可选）
  - `abstract`: 摘要

### 2. JSON 文件存储
- **文件路径**: `./docs/cv-arxiv-daily.json`
- **格式**: 按类别组织的字典结构
- **用途**: 作为备份和兼容性支持

## 📊 当前数据统计

根据数据库查询结果：
- **Manipulation**: 1,164 篇
- **VLM**: 2,177 篇
- **VLA**: 498 篇
- **Humanoid**: 279 篇
- **Dexterous**: 205 篇
- **总计**: 4,323 篇

## 🔧 代码位置

- **主抓取逻辑**: `daily_arxiv.py`
  - `get_daily_papers()`: 从 ArXiv 获取论文
  - `demo()`: 主抓取流程
  - `update_json_file()`: 更新JSON文件并保存到数据库

- **API 端点**: `app.py`
  - `/api/fetch`: 触发手动抓取
  - `/api/fetch-status`: 获取抓取状态

- **配置文件**: `config.yaml`
  - 关键词配置
  - 抓取参数

## ⚠️ 注意事项

1. **速率限制**: ArXiv API 有速率限制，已通过延迟和重试机制处理
2. **数据去重**: 通过 ArXiv ID 自动去重，相同ID的论文会更新而不是重复添加
3. **代码链接**: 优先从论文的 comments 字段提取，如果没有则标记为 null
4. **错误处理**: 抓取过程中如果遇到错误，会继续处理已获取的数据，不会完全失败

## 🚀 使用示例

### 手动触发抓取（通过 API）

```bash
curl -X POST http://localhost:5001/api/fetch \
  -H "Content-Type: application/json" \
  -d '{"max_results": 20, "config_path": "config.yaml"}'
```

### 命令行抓取

```bash
python3 daily_arxiv.py --config_path config.yaml
```

## ✅ 已实现的优化

1. ✅ **扩展关键词**: 已按照具身智能研究方向全量标签配置（9个研究方向）
2. ✅ **智能去重**: 已实现基于标题相似度的去重机制（相似度阈值0.85）
3. ✅ **增量更新**: 已实现只抓取新发布论文的功能，大幅提高效率

详细说明请查看：[论文抓取逻辑优化说明](./论文抓取逻辑优化_20251208.md)

## 📝 未来改进方向

1. **多数据源**: 支持从其他论文库抓取（如 Papers with Code）
2. **智能分类**: 基于标题和摘要自动分类论文
3. **相似度优化**: 使用更先进的NLP模型计算相似度
4. **并行抓取**: 支持多线程/多进程抓取，进一步提高效率

