# 数据健康检查与修复方案

## 概述

本文档详细分析了网站数据流转路径，识别了所有数据层面的问题，并提供了完整的修复方案。

## 一、数据流转路径分析

### 1.1 论文数据流转路径

```
ArXiv API 
  ↓
daily_arxiv.py (get_daily_papers)
  ↓
update_json_file (保存到JSON和数据库)
  ↓
save_paper_to_db.py (去重和保存)
  ↓
数据库 (papers.db)
  ↓
app.py (/api/papers)
  ↓
前端 (index.html)
```

**关键节点：**
- **数据源**: ArXiv API
- **抓取脚本**: `fetch_new_data.py` → `daily_arxiv.py`
- **保存逻辑**: `save_paper_to_db.py`
- **定时任务**: `app.py` 的 `start_scheduler()` → `fetch_papers()`
- **API接口**: `/api/papers`
- **前端展示**: `templates/index.html`

### 1.2 B站数据流转路径

```
B站API (bilibili-api-python)
  ↓
bilibili_client.py (BilibiliClient)
  ↓
fetch_bilibili_data.py (fetch_and_save_up_data)
  ↓
数据库 (bilibili.db)
  ↓
app.py (/api/bilibili/all)
  ↓
前端 (bilibili.html)
```

**关键节点：**
- **数据源**: B站API (通过 bilibili-api-python)
- **客户端**: `bilibili_client.py`
- **抓取脚本**: `fetch_bilibili_data.py`
- **定时任务**: ❌ **未配置**
- **API接口**: `/api/bilibili/all`
- **前端展示**: `templates/bilibili.html`

### 1.3 新闻数据流转路径

```
新闻API (NewsAPI, RSS等)
  ↓
news_client.py / rss_news_client.py
  ↓
fetch_news.py (fetch_and_save_news)
  ↓
数据库 (news.db)
  ↓
app.py (/api/news)
  ↓
前端 (index.html)
```

### 1.4 招聘数据流转路径

```
GitHub Jobs API
  ↓
github_jobs_client.py
  ↓
fetch_jobs.py (fetch_and_save_jobs)
  ↓
数据库 (jobs.db)
  ↓
app.py (/api/jobs)
  ↓
前端 (index.html)
```

## 二、发现的问题

### 2.1 论文数据问题

#### 问题1: 定时任务可能未正确执行
- **位置**: `app.py` 的 `start_scheduler()` 函数
- **现象**: 12月16日的论文没有数据
- **原因分析**:
  1. 定时任务配置为每小时执行 (`0 * * * *`)
  2. 但可能因为环境变量 `AUTO_FETCH_ENABLED` 未设置或为 `false`
  3. 或者定时任务执行失败但没有告警机制

#### 问题2: days_back 配置可能导致漏抓
- **位置**: `fetch_new_data.py` 第27行
- **配置**: `days_back=14` (只抓取最近14天的论文)
- **问题**: 如果定时任务某次执行失败，可能漏掉当天的论文

#### 问题3: 缺少数据健康检查机制
- **问题**: 没有自动检查数据是否及时更新的机制
- **影响**: 数据过期无法及时发现

### 2.2 B站数据问题

#### 问题1: 没有配置定时任务 ⚠️ **严重**
- **位置**: `app.py` 的 `start_scheduler()` 函数
- **现象**: B站数据需要手动运行 `fetch_bilibili_data.py` 才能更新
- **影响**: 前端显示的数据不是最新的

#### 问题2: API风控处理不够完善
- **位置**: `bilibili_client.py`
- **现象**: 请求B站API被风控(412错误)
- **现有处理**:
  - 有重试机制 (retry参数)
  - 有fallback方法 (`_fallback_user_info`, `_fallback_user_videos`)
  - 有延迟机制 (sleep)
- **问题**:
  1. 延迟时间可能不够 (1.5秒可能太短)
  2. 没有指数退避策略
  3. 没有请求频率限制器
  4. 错误信息没有持久化到数据库

#### 问题3: 缓存机制可能导致显示旧数据
- **位置**: `app.py` 第114-122行
- **配置**: `BILIBILI_CACHE_DURATION = 600` (10分钟缓存)
- **问题**: 如果API请求失败，会返回过期缓存，前端无法知道数据是否最新

### 2.3 数据更新及时性问题

#### 问题1: 缺少数据新鲜度检查
- **问题**: 没有机制检查数据是否在合理时间内更新
- **影响**: 数据过期无法及时发现

#### 问题2: 缺少失败重试机制
- **问题**: 如果某次抓取失败，不会自动重试
- **影响**: 可能导致数据缺失

## 三、修复方案

### 3.1 论文数据修复方案

#### 修复1: 确保定时任务正确执行

**步骤1**: 检查环境变量配置
```bash
# 在 .env 文件中确保设置
AUTO_FETCH_ENABLED=true
AUTO_FETCH_SCHEDULE="0 * * * *"  # 每小时整点执行
```

**步骤2**: 添加定时任务执行日志
- 在 `app.py` 的 `scheduled_fetch()` 函数中添加详细日志
- 记录每次执行的时间、结果、新增论文数等

**步骤3**: 添加失败告警机制
- 如果定时任务执行失败，记录错误日志
- 可以集成邮件或Webhook通知

#### 修复2: 优化 days_back 配置

**方案**: 增加 days_back 到 21 天，确保不会漏抓
```python
# fetch_new_data.py
config['days_back'] = 21  # 从14天增加到21天
```

**原因**: 
- ArXiv论文通常在提交后1-2天才会出现在搜索结果中
- 14天可能不够，增加到21天更安全

#### 修复3: 添加数据健康检查

**方案**: 使用 `scripts/data_health_check.py` 脚本定期检查数据健康状态

**执行方式**:
```bash
# 手动执行
python3 scripts/data_health_check.py

# 添加到定时任务 (每天凌晨2点执行)
# 在 app.py 的 start_scheduler() 中添加
```

### 3.2 B站数据修复方案

#### 修复1: 添加B站数据定时任务 ⚠️ **重要**

**步骤1**: 在 `app.py` 的 `start_scheduler()` 函数中添加B站数据抓取任务

```python
def scheduled_fetch_bilibili():
    """定时抓取B站数据任务"""
    try:
        logger.info("=" * 60)
        logger.info("开始执行定时B站数据抓取任务...")
        logger.info("=" * 60)
        from fetch_bilibili_data import fetch_all_bilibili_data
        fetch_all_bilibili_data(video_count=50, delay_between_requests=2.0)
        logger.info("=" * 60)
        logger.info("定时B站数据抓取任务完成")
        logger.info("=" * 60)
    except Exception as e:
        logger.error("=" * 60)
        logger.error(f"定时B站数据抓取任务失败: {e}")
        logger.error("=" * 60)
        import traceback
        logger.error(traceback.format_exc())

# 配置B站数据抓取定时任务（每6小时执行一次，避免触发风控）
bilibili_schedule_cron = os.getenv('AUTO_FETCH_BILIBILI_SCHEDULE', '0 */6 * * *')  # 每6小时执行
if bilibili_schedule_cron:
    cron_list = bilibili_schedule_cron.split(';')
    for idx, cron_expr in enumerate(cron_list):
        cron_expr = cron_expr.strip()
        if not cron_expr:
            continue
        parts = cron_expr.split()
        if len(parts) == 5:
            minute, hour, day, month, weekday = parts
            job_id = f'hourly_fetch_bilibili_{idx}'
            job_name = f'B站数据抓取_{idx+1}'
            scheduler.add_job(
                scheduled_fetch_bilibili,
                trigger=CronTrigger(
                    minute=minute,
                    hour=hour,
                    day=day,
                    month=month,
                    day_of_week=weekday
                ),
                id=job_id,
                name=job_name,
                replace_existing=True
            )
            logger.info(f"B站数据抓取定时任务已配置 ({job_name}): {cron_expr}")
```

**步骤2**: 在环境变量中配置
```bash
# .env 文件
AUTO_FETCH_BILIBILI_SCHEDULE="0 */6 * * *"  # 每6小时执行一次
```

#### 修复2: 优化B站API风控处理

**步骤1**: 增加请求延迟和指数退避

```python
# bilibili_client.py
def _request_json(self, url: str, params: Optional[Dict] = None, timeout: int = 10, retry: int = 3) -> Optional[Dict]:
    # 增加重试次数到3次
    # 使用指数退避: 2秒, 4秒, 8秒
    for attempt in range(retry + 1):
        if attempt > 0:
            delay = 2.0 * (2 ** (attempt - 1))  # 指数退避
            logger.debug(f"HTTP请求重试 {attempt+1}/{retry+1}，等待 {delay}秒...")
            time.sleep(delay)
        
        # ... 现有代码 ...
        
        if resp.status_code == 412:
            logger.warning(f"检测到412风控，等待更长时间后重试...")
            if attempt < retry:
                time.sleep(10.0 * (attempt + 1))  # 10秒, 20秒, 30秒
                continue
```

**步骤2**: 添加请求频率限制器

```python
# bilibili_client.py
import time
from collections import deque

class BilibiliClient:
    def __init__(self, timeout=10, min_request_interval=2.0):
        self.timeout = timeout
        self.min_request_interval = min_request_interval
        self.last_request_time = 0
    
    def _rate_limit(self):
        """请求频率限制"""
        current_time = time.time()
        time_since_last = current_time - self.last_request_time
        if time_since_last < self.min_request_interval:
            sleep_time = self.min_request_interval - time_since_last
            time.sleep(sleep_time)
        self.last_request_time = time.time()
    
    def get_user_info(self, mid: int, retry: int = 3) -> Optional[Dict]:
        self._rate_limit()  # 添加频率限制
        # ... 现有代码 ...
```

**步骤3**: 改进错误处理和持久化

```python
# fetch_bilibili_data.py
def fetch_and_save_up_data(uid, video_count=50):
    try:
        # ... 现有代码 ...
    except Exception as e:
        logger.error(f"抓取UP主 {uid} 数据失败: {e}")
        
        # 改进错误信息记录
        up = session.query(BilibiliUp).filter_by(uid=uid).first()
        if up:
            up.fetch_error = f"{datetime.now().strftime('%Y-%m-%d %H:%M:%S')}: {str(e)}"
            up.last_fetch_at = datetime.now()
            session.commit()
        
        # 如果是412错误，等待更长时间后重试
        if "412" in str(e) or "风控" in str(e):
            logger.warning(f"检测到风控，等待30秒后重试...")
            time.sleep(30)
            # 可以在这里添加重试逻辑
```

#### 修复3: 改进缓存机制

**方案**: 在API响应中添加数据新鲜度标记

```python
# app.py /api/bilibili/all
response_data = {
    'success': True,
    'data': all_data,
    'total': len(all_data),
    'updated_at': datetime.now().isoformat(),
    'source': 'database',
    'data_freshness': {  # 添加数据新鲜度信息
        'oldest_update': oldest_update.isoformat() if oldest_update else None,
        'hours_since_update': hours_since_update if oldest_update else None,
        'is_stale': hours_since_update > 25 if oldest_update else True
    }
}
```

### 3.3 数据更新及时性修复方案

#### 修复1: 添加数据健康检查定时任务

**方案**: 在 `app.py` 的 `start_scheduler()` 中添加数据健康检查任务

```python
def scheduled_health_check():
    """定时数据健康检查任务"""
    try:
        logger.info("开始执行数据健康检查...")
        from scripts.data_health_check import DataHealthChecker
        checker = DataHealthChecker()
        checker.check_papers_data()
        checker.check_bilibili_data()
        checker.check_news_data()
        report = checker.generate_report()
        
        # 如果有严重问题，记录告警
        if report['issue_count'] > 0:
            logger.error(f"数据健康检查发现 {report['issue_count']} 个问题")
            # 可以在这里添加告警通知
        
    except Exception as e:
        logger.error(f"数据健康检查失败: {e}")

# 每天凌晨2点执行健康检查
scheduler.add_job(
    scheduled_health_check,
    trigger=CronTrigger(hour=2, minute=0),
    id='daily_health_check',
    name='每天数据健康检查',
    replace_existing=True
)
```

#### 修复2: 添加失败重试机制

**方案**: 在定时任务中添加失败重试逻辑

```python
def scheduled_fetch_with_retry(max_retries=3):
    """带重试的论文抓取任务"""
    for attempt in range(max_retries):
        try:
            from fetch_new_data import fetch_papers
            fetch_papers()
            logger.info("论文抓取成功")
            return
        except Exception as e:
            logger.error(f"论文抓取失败 (尝试 {attempt+1}/{max_retries}): {e}")
            if attempt < max_retries - 1:
                wait_time = 5 * (attempt + 1)  # 5秒, 10秒, 15秒
                logger.info(f"等待 {wait_time} 秒后重试...")
                time.sleep(wait_time)
            else:
                logger.error("论文抓取最终失败，已重试所有次数")
                # 可以在这里添加告警通知
```

## 四、实施步骤

### 步骤1: 立即修复 (高优先级)

1. **添加B站数据定时任务**
   - 修改 `app.py` 的 `start_scheduler()` 函数
   - 添加 `scheduled_fetch_bilibili()` 函数
   - 配置定时任务 (每6小时执行一次)

2. **优化B站API风控处理**
   - 修改 `bilibili_client.py` 增加请求延迟
   - 添加指数退避策略
   - 改进错误处理

3. **运行数据健康检查**
   - 执行 `python3 scripts/data_health_check.py`
   - 查看报告，识别当前问题

### 步骤2: 短期修复 (中优先级)

1. **优化论文抓取配置**
   - 将 `days_back` 从14天增加到21天
   - 添加定时任务执行日志

2. **添加数据健康检查定时任务**
   - 在 `app.py` 中添加健康检查任务
   - 每天凌晨2点执行

3. **改进缓存机制**
   - 在API响应中添加数据新鲜度标记
   - 前端可以根据标记显示数据状态

### 步骤3: 长期优化 (低优先级)

1. **添加告警机制**
   - 集成邮件或Webhook通知
   - 数据异常时自动告警

2. **添加监控面板**
   - 在管理后台添加数据监控面板
   - 实时显示数据健康状态

3. **优化数据抓取策略**
   - 根据数据更新频率动态调整抓取间隔
   - 实现智能重试机制

## 五、验证方法

### 5.1 验证论文数据更新

```bash
# 1. 检查定时任务是否执行
grep "定时论文抓取任务" logs/app.log

# 2. 检查数据库中最新论文日期
python3 -c "
from models import get_session, Paper
from sqlalchemy import func
session = get_session()
latest = session.query(Paper).order_by(Paper.publish_date.desc()).first()
print(f'最新论文日期: {latest.publish_date}')
session.close()
"

# 3. 检查今天是否有新论文
python3 -c "
from models import get_session, Paper
from datetime import datetime, date
session = get_session()
today = date.today()
today_start = datetime.combine(today, datetime.min.time())
count = session.query(Paper).filter(Paper.created_at >= today_start).count()
print(f'今天新增论文: {count}篇')
session.close()
"
```

### 5.2 验证B站数据更新

```bash
# 1. 检查B站数据最后更新时间
python3 -c "
from bilibili_models import get_bilibili_session, BilibiliUp
from datetime import datetime
session = get_bilibili_session()
ups = session.query(BilibiliUp).filter_by(is_active=True).all()
for up in ups:
    if up.last_fetch_at:
        hours_ago = (datetime.now() - up.last_fetch_at).total_seconds() / 3600
        print(f'{up.name}: {up.last_fetch_at} ({hours_ago:.1f}小时前)')
session.close()
"

# 2. 手动触发B站数据抓取
python3 fetch_bilibili_data.py

# 3. 检查API返回的数据新鲜度
curl http://localhost:5001/api/bilibili/all | jq '.data_freshness'
```

### 5.3 运行完整健康检查

```bash
# 运行数据健康检查脚本
python3 scripts/data_health_check.py

# 查看退出码
echo $?
# 0: 正常
# 1: 有严重问题
# 2: 有警告
```

## 六、监控和维护

### 6.1 日常监控

1. **每天检查数据健康报告**
   - 查看 `scripts/data_health_check.py` 的输出
   - 关注警告和问题

2. **每周检查日志**
   - 查看定时任务执行日志
   - 检查是否有频繁失败

3. **每月检查数据质量**
   - 检查数据完整性
   - 检查数据准确性

### 6.2 故障处理

1. **论文数据未更新**
   - 检查定时任务是否执行
   - 检查 `AUTO_FETCH_ENABLED` 环境变量
   - 手动执行 `python3 fetch_new_data.py --papers`

2. **B站数据未更新**
   - 检查定时任务是否配置
   - 检查API是否被风控
   - 手动执行 `python3 fetch_bilibili_data.py`

3. **API被风控**
   - 增加请求延迟
   - 检查Cookie配置
   - 等待一段时间后重试

## 七、总结

### 关键问题总结

1. **B站数据没有定时任务** ⚠️ **最严重**
2. **论文数据定时任务可能未正确执行**
3. **B站API风控处理不够完善**
4. **缺少数据健康检查机制**

### 修复优先级

1. **P0 (立即修复)**: 添加B站数据定时任务
2. **P1 (高优先级)**: 优化B站API风控处理
3. **P2 (中优先级)**: 添加数据健康检查
4. **P3 (低优先级)**: 优化论文抓取配置

### 预期效果

修复后，系统将具备：
- ✅ 所有数据源都有定时自动更新
- ✅ 完善的错误处理和重试机制
- ✅ 数据健康监控和告警
- ✅ 数据新鲜度可追踪
- ✅ 前端可以显示数据状态

