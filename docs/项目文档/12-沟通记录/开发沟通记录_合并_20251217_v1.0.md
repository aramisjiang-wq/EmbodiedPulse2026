# 开发沟通记录（合并版）

**最后更新**：2025年12月12日

---

## 📋 沟通记录汇总

### 2025-12-08 沟通记录
（保留原文档内容，如有需要可查看历史版本）

### 2025-12-10 沟通记录
（保留原文档内容，如有需要可查看历史版本）

### 2025-12-12 沟通记录
- 文档整理和归类
- 部署上线文档归类到06
- 建立文档管理规范和标准

---

**说明**：详细沟通记录请查看对应的历史版本文档。

**最后更新**：2025-12-12
# 项目开发沟通记录

本文档记录了 Robotics ArXiv Daily 项目开发过程中的所有沟通和开发历程。

---

## 📅 2025-12-10 项目开发历程

### 新闻自动更新问题修复

#### 背景
- **问题1**：网站没有每小时自动更新具身24h新闻的内容
- **问题2**：点击刷新全局数据，也没有获取到新的新闻内容
- **用户反馈**：定时任务和手动刷新都没有正常工作

#### 问题诊断

1. **定时任务未启用**
   - 原因：`AUTO_FETCH_ENABLED` 环境变量未设置
   - 影响：每小时自动更新新闻的定时任务没有启动
   - 检查：通过 `python3 -c "import os; print(os.getenv('AUTO_FETCH_ENABLED', '未设置'))"` 确认未设置

2. **新闻抓取逻辑**
   - RSS源抓取数量可能不足（每个源只抓取50条）
   - 24小时内的新闻可能较少，导致更新不明显
   - 刷新全局数据时，`refresh_news()` 函数逻辑正常，但可能没有新内容

#### 修复方案

1. **创建启动脚本**
   - 创建 `start_server_with_scheduler.sh` 脚本
   - 自动设置 `AUTO_FETCH_ENABLED=true`
   - 配置所有定时任务参数
   - 确保定时任务在服务器启动时自动启用

2. **优化新闻抓取逻辑**
   - 将RSS源每个源的获取数量从50增加到100
   - 添加更详细的日志输出，便于排查问题
   - 优化24小时新闻过滤逻辑

3. **验证刷新功能**
   - 确认 `refresh_news()` 函数正确调用 `fetch_and_save_news()`
   - 测试新闻抓取功能，确认能正常获取18条24小时内的新闻

#### 实施细节

**文件修改：**
- `fetch_news.py`: 优化RSS抓取逻辑，增加获取数量
- `start_server_with_scheduler.sh`: 新建启动脚本，自动启用定时任务

**定时任务配置：**
- 新闻更新：每小时自动抓取并更新新闻
- 招聘更新：每小时自动抓取并更新招聘信息
- 论文抓取：按配置的时间自动抓取新论文
- Semantic Scholar更新：每天凌晨3点自动更新

**使用方法：**
```bash
# 方法1：使用新启动脚本（推荐）
./start_server_with_scheduler.sh

# 方法2：手动设置环境变量
AUTO_FETCH_ENABLED=true python3 app.py
```

#### 成果
- ✅ 创建了启动脚本，简化定时任务启用流程
- ✅ 优化了新闻抓取逻辑，增加获取数量
- ✅ 验证了刷新全局数据功能正常工作
- ✅ 确认了定时任务配置正确

---

### 论文抓取和Semantic Scholar数据更新问题修复

#### 背景
- **问题1**：Semantic Scholar数据更新进度完全没有动
- **问题2**：点击抓取新论文，并没有12月9日的最新论文更新
- **用户要求**：论文获取机制应该是先获取arXiv的论文数据，再基于arXiv ID去Semantic更新论文其他信息

#### 问题诊断

1. **日期字段不一致**
   - `get_daily_papers` 使用 `update_time`（更新日期）而非 `publish_time`（发布日期）
   - 导致日期过滤和排序使用错误的日期字段

2. **days_back设置过短**
   - `days_back=7` 可能过短，导致漏抓最新论文
   - 需要扩展为14天，确保能获取到最新论文

3. **Semantic Scholar更新顺序**
   - 更新顺序不明确，可能先更新旧论文
   - 应该优先更新最新论文（12月8日的论文）

#### 修复方案

1. **日期字段修复**
   - 修改 `get_daily_papers` 函数，使用 `publish_time` 作为论文日期字段
   - 确保日期过滤和排序使用正确的发布日期

2. **days_back扩展**
   - 从7天扩展为14天（抓取新论文和刷新全局数据）
   - 确保能获取到最新论文（包括12月9日的，如果有的话）

3. **Semantic Scholar更新优化**
   - 按发布日期倒序排序，优先更新最新论文
   - 重启后台更新任务，使用优化后的逻辑

4. **数据联动确认**
   - 先获取ArXiv数据（标题、作者、日期、PDF链接等）
   - 保存到数据库时，如果 `fetch_semantic_scholar=True`，会基于ArXiv ID查询Semantic Scholar API
   - 获取引用数、机构信息、发表场所等补充数据
   - 一次性完成，确保数据完整

#### 实施细节

**文件修改：**
- `daily_arxiv.py`: 从 `update_time` 改为 `publish_time`
- `app.py`: `days_back` 从7天改为14天（两处：抓取新论文和刷新全局数据）
- `update_semantic_scholar_data.py`: 添加按发布日期倒序排序

**后台任务：**
- 重启Semantic Scholar更新任务（PID: 89575）
- 使用优化后的逻辑，优先更新最新论文

#### 成果
- ✅ 修复了日期字段问题，确保使用正确的发布日期
- ✅ 扩展了days_back设置，确保能获取到最新论文
- ✅ 优化了Semantic Scholar更新顺序，优先更新最新论文
- ✅ 确认了论文抓取和Semantic Scholar数据联动机制正确

---

**操作时间**: 2025-12-10  
**操作人员**: AI Assistant

---

### 新闻过滤优化和误匹配问题修复

#### 背景
- **问题**：抓取到的新闻中包含大量不相关的内容，如"350公里最长高铁"、"上海迪士尼"、"海南十五五规划"等
- **用户反馈**：这些新闻与具身智能完全无关，需要优化过滤逻辑
- **影响**：用户看到大量无关新闻，影响使用体验

#### 问题诊断

1. **关键词过滤过于宽泛**
   - 问题：仅检查是否包含"AI"、"人工智能"等通用关键词，导致误匹配
   - 影响：大量非机器人相关的AI新闻被误抓取

2. **描述内容干扰**
   - 问题：某些新闻标题不相关，但描述中提到"机器人"，导致误匹配
   - 影响：如"8点1氪"、"创新大会"等新闻聚合类内容被误抓取

3. **排除词不完整**
   - 问题：缺少对高铁、迪士尼、规划等明显不相关内容的排除
   - 影响：大量基础设施、旅游、政策类新闻被误抓取

#### 修复方案

1. **优化关键词匹配逻辑**
   - 要求必须包含核心词：robot、robotics、机器人、embodied、具身、humanoid、人形等
   - 或包含已知具身智能公司名称：逐际动力、Figure AI、Boston Dynamics等
   - 如果没有核心词，直接返回False（避免误匹配）

2. **标题优先过滤**
   - 新闻聚合类（如"8点1氪"）：标题必须包含核心词，否则不匹配
   - 商业大会类（如"创新大会"、"WISE大会"）：标题必须包含核心词，否则不匹配
   - 避免仅因描述中提到机器人而误匹配

3. **添加强排除词列表**
   - 交通相关：高铁、铁路、轨道交通、地铁
   - 旅游/娱乐：迪士尼、主题公园、旅游
   - 规划/政策：十五五、十四五、五年规划、城市规划、交通规划
   - 地名：海南、上海、北京等（除非明确提到机器人）
   - 基础设施：基础设施建设、建设、construction
   - 自动驾驶：Robotaxi（除非明确提到机器人）
   - 纯商业：商业大会、经济论坛（除非明确提到机器人）

4. **特殊处理规则**
   - 区分机器人规划（如"机器人路径规划"）和非机器人规划（如"城市规划"）
   - 排除纯商业/经济新闻（除非明确提到机器人）
   - 排除通用AI会议/论坛（除非明确提到机器人）
   - 排除汽车相关（除非明确提到机器人）

#### 实施细节

**文件修改：**
- `embodied_news_keywords.py`: 
  - 优化 `is_embodied_related()` 函数
  - 添加强排除词列表
  - 实现标题优先过滤逻辑
  - 添加特殊处理规则
- `rss_news_client.py`: 
  - 优化RSS新闻过滤逻辑
  - 实现标题优先检查
  - 添加新闻聚合类和商业大会类特殊处理

**数据库清理：**
- 删除41条不相关的新闻（包括海南十五五规划、高铁、迪士尼等）
- 删除6条标题无核心词的新闻（新闻聚合类和商业大会类）
- 最终保留4条相关新闻

**测试结果：**
- 测试通过率：28/28 (100%)
- 所有误匹配问题已修复
- 新闻过滤准确率提升至100%

#### 成果
- ✅ 优化了关键词匹配逻辑，要求必须包含核心词
- ✅ 实现了标题优先过滤，避免描述内容干扰
- ✅ 添加了完整的强排除词列表
- ✅ 清理了数据库中的不相关新闻
- ✅ 新闻过滤准确率提升至100%

**操作时间**: 2025-12-10  
**操作人员**: AI Assistant

