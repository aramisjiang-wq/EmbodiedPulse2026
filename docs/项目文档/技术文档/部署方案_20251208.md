# Robotics ArXiv Daily - éƒ¨ç½²æ–¹æ¡ˆæ–‡æ¡£

## ğŸ“‹ ç›®å½•

1. [éƒ¨ç½²æ¶æ„åˆ†æ](#éƒ¨ç½²æ¶æ„åˆ†æ)
2. [éƒ¨ç½²å‰å‡†å¤‡](#éƒ¨ç½²å‰å‡†å¤‡)
3. [æ–¹æ¡ˆä¸€ï¼šä¼ ç»ŸæœåŠ¡å™¨éƒ¨ç½²ï¼ˆæ¨èï¼‰](#æ–¹æ¡ˆä¸€ä¼ ç»ŸæœåŠ¡å™¨éƒ¨ç½²æ¨è)
4. [æ–¹æ¡ˆäºŒï¼šDockerå®¹å™¨åŒ–éƒ¨ç½²](#æ–¹æ¡ˆäºŒdockerå®¹å™¨åŒ–éƒ¨ç½²)
5. [æ–¹æ¡ˆä¸‰ï¼šäº‘å¹³å°éƒ¨ç½²](#æ–¹æ¡ˆä¸‰äº‘å¹³å°éƒ¨ç½²)
6. [æ•°æ®åº“è¿ç§»ä¸å¤‡ä»½](#æ•°æ®åº“è¿ç§»ä¸å¤‡ä»½)
7. [é™æ€èµ„æºä¼˜åŒ–](#é™æ€èµ„æºä¼˜åŒ–)
8. [å®‰å…¨æ€§é…ç½®](#å®‰å…¨æ€§é…ç½®)
9. [ç›‘æ§ä¸ç»´æŠ¤](#ç›‘æ§ä¸ç»´æŠ¤)
10. [æ•…éšœæ’æŸ¥](#æ•…éšœæ’æŸ¥)

---

## éƒ¨ç½²æ¶æ„åˆ†æ

### å½“å‰æŠ€æœ¯æ ˆ

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚           ç”¨æˆ·æµè§ˆå™¨                      â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
               â”‚ HTTP/HTTPS
               â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚         Nginx (åå‘ä»£ç†)                 â”‚
â”‚     - SSL/TLSç»ˆæ­¢                        â”‚
â”‚     - é™æ€æ–‡ä»¶æœåŠ¡                       â”‚
â”‚     - è´Ÿè½½å‡è¡¡ï¼ˆå¯é€‰ï¼‰                   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
               â”‚
               â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚      Gunicorn (WSGIæœåŠ¡å™¨)               â”‚
â”‚     - å¤šè¿›ç¨‹å¤„ç†                        â”‚
â”‚     - è¿›ç¨‹ç®¡ç†                          â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
               â”‚
               â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚         Flaskåº”ç”¨ (app.py)               â”‚
â”‚     - APIè·¯ç”±                           â”‚
â”‚     - ä¸šåŠ¡é€»è¾‘                          â”‚
â”‚     - APSchedulerå®šæ—¶ä»»åŠ¡               â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
               â”‚
               â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚         SQLiteæ•°æ®åº“                     â”‚
â”‚     - papers.db                         â”‚
â”‚     - è®ºæ–‡æ•°æ®å­˜å‚¨                      â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### ç»„ä»¶è¯´æ˜

| ç»„ä»¶ | ä½œç”¨ | ç”Ÿäº§ç¯å¢ƒè¦æ±‚ |
|------|------|------------|
| **Nginx** | åå‘ä»£ç†ã€é™æ€æ–‡ä»¶æœåŠ¡ã€SSL | å¿…éœ€ |
| **Gunicorn** | WSGIæœåŠ¡å™¨ï¼Œå¤šè¿›ç¨‹å¤„ç† | å¿…éœ€ |
| **Flask** | Webæ¡†æ¶ï¼Œä¸šåŠ¡é€»è¾‘ | å¿…éœ€ |
| **SQLite** | æ•°æ®åº“ï¼ˆå¯å‡çº§PostgreSQLï¼‰ | å¿…éœ€ |
| **APScheduler** | å®šæ—¶ä»»åŠ¡è°ƒåº¦ | å¯é€‰ |

---

## éƒ¨ç½²å‰å‡†å¤‡

### 1. ä»£ç å‡†å¤‡

```bash
# 1. ç¡®ä¿ä»£ç å·²æäº¤åˆ°Gitä»“åº“
git add .
git commit -m "å‡†å¤‡éƒ¨ç½²"
git push origin main

# 2. æ£€æŸ¥å…³é”®æ–‡ä»¶
ls -la app.py models.py requirements.txt config.yaml
ls -la templates/ static/
```

### 2. ç¯å¢ƒå˜é‡é…ç½®

åˆ›å»º `.env` æ–‡ä»¶ï¼ˆ**ä¸è¦æäº¤åˆ°Git**ï¼‰ï¼š

```bash
# æ•°æ®åº“é…ç½®
DATABASE_URL=sqlite:///./papers.db

# æœåŠ¡å™¨é…ç½®
PORT=5001
FLASK_ENV=production
DEBUG=false

# è‡ªåŠ¨æŠ“å–é…ç½®
AUTO_FETCH_ENABLED=true
AUTO_FETCH_SCHEDULE=0 2 * * *
AUTO_FETCH_MAX_RESULTS=10

# æ—¥å¿—é…ç½®
LOG_LEVEL=INFO
```

### 3. ä¾èµ–æ£€æŸ¥

```bash
# æ£€æŸ¥requirements.txtæ˜¯å¦å®Œæ•´
cat requirements.txt

# åº”è¯¥åŒ…å«ï¼š
# - flask
# - gunicorn (ç”Ÿäº§ç¯å¢ƒå¿…éœ€)
# - sqlalchemy
# - apscheduler
# - arxiv
# - pyyaml
# - requests
```

### 4. æ•°æ®åº“å¤‡ä»½

```bash
# å¤‡ä»½ç°æœ‰æ•°æ®åº“
cp papers.db papers.db.backup

# å¯¼å‡ºæ•°æ®ï¼ˆå¯é€‰ï¼‰
sqlite3 papers.db .dump > papers_backup.sql
```

---

## æ–¹æ¡ˆä¸€ï¼šä¼ ç»ŸæœåŠ¡å™¨éƒ¨ç½²ï¼ˆæ¨èï¼‰

### é€‚ç”¨åœºæ™¯
- è‡ªæœ‰VPS/äº‘æœåŠ¡å™¨
- éœ€è¦å®Œå…¨æ§åˆ¶
- æˆæœ¬æ•æ„Ÿ
- é€‚åˆä¸­å°å‹åº”ç”¨

### æœåŠ¡å™¨è¦æ±‚

**æœ€ä½é…ç½®**ï¼š
- CPU: 1æ ¸
- å†…å­˜: 1GB
- å­˜å‚¨: 10GB
- å¸¦å®½: 1Mbps

**æ¨èé…ç½®**ï¼š
- CPU: 2æ ¸
- å†…å­˜: 2GB
- å­˜å‚¨: 20GB SSD
- å¸¦å®½: 5Mbps

### éƒ¨ç½²æ­¥éª¤

#### 1. æœåŠ¡å™¨ç¯å¢ƒå‡†å¤‡

```bash
# æ›´æ–°ç³»ç»Ÿ
sudo apt update && sudo apt upgrade -y

# å®‰è£…Python 3.9+
sudo apt install python3 python3-pip python3-venv -y

# å®‰è£…Nginx
sudo apt install nginx -y

# å®‰è£…Git
sudo apt install git -y

# å®‰è£…SQLiteï¼ˆé€šå¸¸å·²åŒ…å«ï¼‰
sqlite3 --version
```

#### 2. åˆ›å»ºåº”ç”¨ç”¨æˆ·

```bash
# åˆ›å»ºä¸“ç”¨ç”¨æˆ·ï¼ˆå®‰å…¨è€ƒè™‘ï¼‰
sudo useradd -m -s /bin/bash robotics-app
sudo su - robotics-app
```

#### 3. éƒ¨ç½²ä»£ç 

```bash
# å…‹éš†ä»£ç 
cd ~
git clone <your-repo-url> robotics_arxiv_daily
cd robotics_arxiv_daily

# åˆ›å»ºè™šæ‹Ÿç¯å¢ƒ
python3 -m venv venv
source venv/bin/activate

# å®‰è£…ä¾èµ–
pip install --upgrade pip
pip install -r requirements.txt
pip install gunicorn  # ç”Ÿäº§ç¯å¢ƒå¿…éœ€

# åˆå§‹åŒ–æ•°æ®åº“
python3 init_database.py
```

#### 4. é…ç½®Gunicorn

åˆ›å»º `gunicorn_config.py`ï¼š

```python
# gunicorn_config.py
import multiprocessing
import os

# æœåŠ¡å™¨socket
bind = "127.0.0.1:5001"
backlog = 2048

# Workerè¿›ç¨‹
workers = multiprocessing.cpu_count() * 2 + 1
worker_class = "sync"
worker_connections = 1000
timeout = 30
keepalive = 2

# æ—¥å¿—
accesslog = "/var/log/robotics-arxiv/access.log"
errorlog = "/var/log/robotics-arxiv/error.log"
loglevel = "info"

# è¿›ç¨‹å‘½å
proc_name = "robotics-arxiv"

# ç”¨æˆ·å’Œç»„ï¼ˆå¦‚æœä»¥rootè¿è¡Œï¼‰
# user = "robotics-app"
# group = "robotics-app"

# ç¯å¢ƒå˜é‡
raw_env = [
    "FLASK_ENV=production",
    "AUTO_FETCH_ENABLED=true",
    "AUTO_FETCH_SCHEDULE=0 2 * * *",
]
```

#### 5. åˆ›å»ºSystemdæœåŠ¡

åˆ›å»º `/etc/systemd/system/robotics-arxiv.service`ï¼š

```ini
[Unit]
Description=Robotics ArXiv Daily Web Application
After=network.target

[Service]
Type=notify
User=robotics-app
Group=robotics-app
WorkingDirectory=/home/robotics-app/robotics_arxiv_daily
Environment="PATH=/home/robotics-app/robotics_arxiv_daily/venv/bin"
Environment="DATABASE_URL=sqlite:///./papers.db"
Environment="AUTO_FETCH_ENABLED=true"
Environment="AUTO_FETCH_SCHEDULE=0 2 * * *"
Environment="AUTO_FETCH_MAX_RESULTS=10"
Environment="FLASK_ENV=production"
ExecStart=/home/robotics-app/robotics_arxiv_daily/venv/bin/gunicorn \
    --config /home/robotics-app/robotics_arxiv_daily/gunicorn_config.py \
    app:app
ExecReload=/bin/kill -s HUP $MAINPID
Restart=always
RestartSec=10

[Install]
WantedBy=multi-user.target
```

#### 6. é…ç½®Nginx

åˆ›å»º `/etc/nginx/sites-available/robotics-arxiv`ï¼š

```nginx
server {
    listen 80;
    server_name your-domain.com www.your-domain.com;

    # é‡å®šå‘åˆ°HTTPSï¼ˆé…ç½®SSLåå¯ç”¨ï¼‰
    # return 301 https://$server_name$request_uri;

    # æ—¥å¿—
    access_log /var/log/nginx/robotics-arxiv-access.log;
    error_log /var/log/nginx/robotics-arxiv-error.log;

    # é™æ€æ–‡ä»¶
    location /static/ {
        alias /home/robotics-app/robotics_arxiv_daily/static/;
        expires 30d;
        add_header Cache-Control "public, immutable";
    }

    # ä»£ç†åˆ°Gunicorn
    location / {
        proxy_pass http://127.0.0.1:5001;
        proxy_set_header Host $host;
        proxy_set_header X-Real-IP $remote_addr;
        proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for;
        proxy_set_header X-Forwarded-Proto $scheme;
        
        # WebSocketæ”¯æŒï¼ˆå¦‚æœéœ€è¦ï¼‰
        proxy_http_version 1.1;
        proxy_set_header Upgrade $http_upgrade;
        proxy_set_header Connection "upgrade";
        
        # è¶…æ—¶è®¾ç½®
        proxy_connect_timeout 60s;
        proxy_send_timeout 60s;
        proxy_read_timeout 60s;
    }

    # é™åˆ¶è¯·æ±‚å¤§å°
    client_max_body_size 10M;
}
```

å¯ç”¨é…ç½®ï¼š

```bash
sudo ln -s /etc/nginx/sites-available/robotics-arxiv /etc/nginx/sites-enabled/
sudo nginx -t  # æµ‹è¯•é…ç½®
sudo systemctl reload nginx
```

#### 7. é…ç½®SSLï¼ˆLet's Encryptï¼‰

```bash
# å®‰è£…Certbot
sudo apt install certbot python3-certbot-nginx -y

# è·å–SSLè¯ä¹¦
sudo certbot --nginx -d your-domain.com -d www.your-domain.com

# è‡ªåŠ¨ç»­æœŸï¼ˆå·²è‡ªåŠ¨é…ç½®ï¼‰
sudo certbot renew --dry-run
```

#### 8. å¯åŠ¨æœåŠ¡

```bash
# åˆ›å»ºæ—¥å¿—ç›®å½•
sudo mkdir -p /var/log/robotics-arxiv
sudo chown robotics-app:robotics-app /var/log/robotics-arxiv

# å¯åŠ¨æœåŠ¡
sudo systemctl daemon-reload
sudo systemctl start robotics-arxiv
sudo systemctl enable robotics-arxiv

# æ£€æŸ¥çŠ¶æ€
sudo systemctl status robotics-arxiv
```

#### 9. é˜²ç«å¢™é…ç½®

```bash
# å…è®¸HTTPå’ŒHTTPS
sudo ufw allow 80/tcp
sudo ufw allow 443/tcp

# å¯ç”¨é˜²ç«å¢™
sudo ufw enable
```

### ç»´æŠ¤å‘½ä»¤

```bash
# æŸ¥çœ‹æœåŠ¡çŠ¶æ€
sudo systemctl status robotics-arxiv

# æŸ¥çœ‹æ—¥å¿—
sudo journalctl -u robotics-arxiv -f
tail -f /var/log/robotics-arxiv/error.log

# é‡å¯æœåŠ¡
sudo systemctl restart robotics-arxiv

# æ›´æ–°ä»£ç 
cd /home/robotics-app/robotics_arxiv_daily
git pull
source venv/bin/activate
pip install -r requirements.txt
sudo systemctl restart robotics-arxiv
```

---

## æ–¹æ¡ˆäºŒï¼šDockerå®¹å™¨åŒ–éƒ¨ç½²

### é€‚ç”¨åœºæ™¯
- éœ€è¦å¿«é€Ÿéƒ¨ç½²
- ç¯å¢ƒä¸€è‡´æ€§è¦æ±‚é«˜
- å¾®æœåŠ¡æ¶æ„
- å®¹å™¨ç¼–æ’ï¼ˆDocker Compose/Kubernetesï¼‰

### 1. åˆ›å»ºDockerfile

```dockerfile
# Dockerfile
FROM python:3.9-slim

# è®¾ç½®å·¥ä½œç›®å½•
WORKDIR /app

# å®‰è£…ç³»ç»Ÿä¾èµ–
RUN apt-get update && apt-get install -y \
    gcc \
    && rm -rf /var/lib/apt/lists/*

# å¤åˆ¶ä¾èµ–æ–‡ä»¶
COPY requirements.txt .

# å®‰è£…Pythonä¾èµ–
RUN pip install --no-cache-dir -r requirements.txt
RUN pip install gunicorn

# å¤åˆ¶åº”ç”¨ä»£ç 
COPY . .

# åˆå§‹åŒ–æ•°æ®åº“
RUN python3 init_database.py || true

# åˆ›å»ºérootç”¨æˆ·
RUN useradd -m -u 1000 appuser && chown -R appuser:appuser /app
USER appuser

# æš´éœ²ç«¯å£
EXPOSE 5001

# å¥åº·æ£€æŸ¥
HEALTHCHECK --interval=30s --timeout=10s --start-period=5s --retries=3 \
    CMD python3 -c "import requests; requests.get('http://localhost:5001/api/stats')"

# å¯åŠ¨å‘½ä»¤
CMD ["gunicorn", "-w", "4", "-b", "0.0.0.0:5001", "--access-logfile", "-", "--error-logfile", "-", "app:app"]
```

### 2. åˆ›å»ºdocker-compose.yml

```yaml
version: '3.8'

services:
  web:
    build: .
    container_name: robotics-arxiv-web
    ports:
      - "5001:5001"
    volumes:
      # æ•°æ®æŒä¹…åŒ–
      - ./papers.db:/app/papers.db
      - ./docs:/app/docs
      - ./config.yaml:/app/config.yaml
    environment:
      - DATABASE_URL=sqlite:///./papers.db
      - AUTO_FETCH_ENABLED=true
      - AUTO_FETCH_SCHEDULE=0 2 * * *
      - AUTO_FETCH_MAX_RESULTS=10
      - FLASK_ENV=production
    restart: always
    healthcheck:
      test: ["CMD", "python3", "-c", "import requests; requests.get('http://localhost:5001/api/stats')"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 40s

  nginx:
    image: nginx:alpine
    container_name: robotics-arxiv-nginx
    ports:
      - "80:80"
      - "443:443"
    volumes:
      - ./nginx.conf:/etc/nginx/nginx.conf:ro
      - ./static:/usr/share/nginx/html/static:ro
      - ./ssl:/etc/nginx/ssl:ro
    depends_on:
      - web
    restart: always
```

### 3. åˆ›å»º.dockerignore

```
__pycache__
*.pyc
*.pyo
*.pyd
.Python
venv/
env/
*.db
*.db-journal
.git
.gitignore
.DS_Store
*.log
```

### 4. éƒ¨ç½²å‘½ä»¤

```bash
# æ„å»ºé•œåƒ
docker-compose build

# å¯åŠ¨æœåŠ¡
docker-compose up -d

# æŸ¥çœ‹æ—¥å¿—
docker-compose logs -f

# åœæ­¢æœåŠ¡
docker-compose down

# æ›´æ–°éƒ¨ç½²
docker-compose pull
docker-compose up -d --build
```

---

## æ–¹æ¡ˆä¸‰ï¼šäº‘å¹³å°éƒ¨ç½²

### 3.1 Railwayéƒ¨ç½²

**ä¼˜ç‚¹**ï¼šç®€å•å¿«é€Ÿï¼Œè‡ªåŠ¨SSLï¼ŒGité›†æˆ

**æ­¥éª¤**ï¼š

1. æ³¨å†ŒRailwayè´¦å·
2. è¿æ¥GitHubä»“åº“
3. é…ç½®ç¯å¢ƒå˜é‡ï¼š
   ```
   AUTO_FETCH_ENABLED=true
   AUTO_FETCH_SCHEDULE=0 2 * * *
   AUTO_FETCH_MAX_RESULTS=10
   ```
4. Railwayè‡ªåŠ¨æ£€æµ‹å¹¶éƒ¨ç½²

**æ³¨æ„äº‹é¡¹**ï¼š
- SQLiteåœ¨Railwayä¸Šå¯èƒ½ä¸ç¨³å®šï¼Œå»ºè®®ä½¿ç”¨PostgreSQLæ’ä»¶
- éœ€è¦é…ç½®æŒä¹…åŒ–å­˜å‚¨

### 3.2 Renderéƒ¨ç½²

**æ­¥éª¤**ï¼š

1. åˆ›å»º `render.yaml`ï¼š

```yaml
services:
  - type: web
    name: robotics-arxiv
    env: python
    buildCommand: pip install -r requirements.txt && python3 init_database.py
    startCommand: gunicorn -w 4 -b 0.0.0.0:$PORT app:app
    envVars:
      - key: AUTO_FETCH_ENABLED
        value: true
      - key: AUTO_FETCH_SCHEDULE
        value: "0 2 * * *"
```

2. åœ¨Render Dashboardè¿æ¥GitHubä»“åº“
3. è‡ªåŠ¨éƒ¨ç½²

### 3.3 Herokuéƒ¨ç½²

**æ­¥éª¤**ï¼š

1. åˆ›å»º `Procfile`ï¼š

```
web: gunicorn -w 4 -b 0.0.0.0:$PORT app:app
```

2. åˆ›å»º `runtime.txt`ï¼š

```
python-3.9.16
```

3. éƒ¨ç½²ï¼š

```bash
heroku create robotics-arxiv-daily
heroku config:set AUTO_FETCH_ENABLED=true
heroku config:set AUTO_FETCH_SCHEDULE="0 2 * * *"
git push heroku main
```

---

## æ•°æ®åº“è¿ç§»ä¸å¤‡ä»½

### SQLite â†’ PostgreSQLè¿ç§»

å¦‚æœæ•°æ®é‡å¢é•¿ï¼Œå»ºè®®è¿ç§»åˆ°PostgreSQLï¼š

#### 1. å®‰è£…PostgreSQL

```bash
sudo apt install postgresql postgresql-contrib -y
```

#### 2. åˆ›å»ºæ•°æ®åº“

```sql
CREATE DATABASE robotics_arxiv;
CREATE USER robotics_user WITH PASSWORD 'your_password';
GRANT ALL PRIVILEGES ON DATABASE robotics_arxiv TO robotics_user;
```

#### 3. ä¿®æ”¹models.py

```python
# ä½¿ç”¨PostgreSQL
DATABASE_URL = os.getenv('DATABASE_URL', 'postgresql://robotics_user:password@localhost/robotics_arxiv')
```

#### 4. æ•°æ®è¿ç§»

```python
# migrate_to_postgresql.py
from sqlalchemy import create_engine
import pandas as pd

# SQLiteè¿æ¥
sqlite_engine = create_engine('sqlite:///papers.db')

# PostgreSQLè¿æ¥
pg_engine = create_engine('postgresql://user:pass@localhost/robotics_arxiv')

# è¯»å–SQLiteæ•°æ®
df = pd.read_sql('SELECT * FROM papers', sqlite_engine)

# å†™å…¥PostgreSQL
df.to_sql('papers', pg_engine, if_exists='append', index=False)
```

### è‡ªåŠ¨å¤‡ä»½è„šæœ¬

åˆ›å»º `backup.sh`ï¼š

```bash
#!/bin/bash
BACKUP_DIR="/backup/robotics-arxiv"
DATE=$(date +%Y%m%d_%H%M%S)

mkdir -p $BACKUP_DIR

# å¤‡ä»½æ•°æ®åº“
cp papers.db $BACKUP_DIR/papers_$DATE.db

# å‹ç¼©å¤‡ä»½
gzip $BACKUP_DIR/papers_$DATE.db

# åˆ é™¤7å¤©å‰çš„å¤‡ä»½
find $BACKUP_DIR -name "*.db.gz" -mtime +7 -delete

echo "å¤‡ä»½å®Œæˆ: papers_$DATE.db.gz"
```

æ·»åŠ åˆ°Cronï¼š

```bash
# æ¯å¤©å‡Œæ™¨3ç‚¹å¤‡ä»½
0 3 * * * /path/to/backup.sh
```

---

## é™æ€èµ„æºä¼˜åŒ–

### 1. CDNé…ç½®

å°†é™æ€èµ„æºï¼ˆCSS/JS/å›¾ç‰‡ï¼‰ä¸Šä¼ åˆ°CDNï¼š

```html
<!-- ä½¿ç”¨CDN -->
<link rel="stylesheet" href="https://cdn.example.com/static/css/style.css">
<script src="https://cdn.example.com/static/js/app.js"></script>
```

### 2. å‹ç¼©ä¼˜åŒ–

```bash
# å‹ç¼©CSS/JS
npm install -g clean-css-cli uglify-js
cleancss -o static/css/style.min.css static/css/style.css
uglifyjs static/js/app.js -o static/js/app.min.js
```

### 3. æµè§ˆå™¨ç¼“å­˜

Nginxé…ç½®å·²åŒ…å«ç¼“å­˜è®¾ç½®ï¼ˆ30å¤©ï¼‰ã€‚

---

## å®‰å…¨æ€§é…ç½®

### 1. ç¯å¢ƒå˜é‡ä¿æŠ¤

```bash
# ä¸è¦å°†æ•æ„Ÿä¿¡æ¯æäº¤åˆ°Git
echo ".env" >> .gitignore
echo "*.db" >> .gitignore
```

### 2. Flaskå®‰å…¨é…ç½®

åœ¨ `app.py` ä¸­æ·»åŠ ï¼š

```python
app.config['SECRET_KEY'] = os.getenv('SECRET_KEY', 'change-this-in-production')
app.config['SESSION_COOKIE_SECURE'] = True  # HTTPS only
app.config['SESSION_COOKIE_HTTPONLY'] = True
app.config['SESSION_COOKIE_SAMESITE'] = 'Lax'
```

### 3. é˜²ç«å¢™è§„åˆ™

```bash
# åªå…è®¸å¿…è¦ç«¯å£
sudo ufw allow 22/tcp   # SSH
sudo ufw allow 80/tcp   # HTTP
sudo ufw allow 443/tcp # HTTPS
sudo ufw enable
```

### 4. å®šæœŸæ›´æ–°

```bash
# ç³»ç»Ÿæ›´æ–°
sudo apt update && sudo apt upgrade -y

# PythonåŒ…æ›´æ–°
pip list --outdated
pip install --upgrade <package>
```

---

## ç›‘æ§ä¸ç»´æŠ¤

### 1. æ—¥å¿—ç›‘æ§

```bash
# å®æ—¶æŸ¥çœ‹æ—¥å¿—
tail -f /var/log/robotics-arxiv/error.log

# æŸ¥çœ‹è®¿é—®æ—¥å¿—
tail -f /var/log/robotics-arxiv/access.log
```

### 2. æ€§èƒ½ç›‘æ§

å®‰è£…ç›‘æ§å·¥å…·ï¼š

```bash
pip install prometheus-client
```

### 3. å¥åº·æ£€æŸ¥

åˆ›å»ºå¥åº·æ£€æŸ¥ç«¯ç‚¹ï¼š

```python
@app.route('/health')
def health():
    try:
        session = get_session()
        session.query(Paper).limit(1).all()
        session.close()
        return jsonify({'status': 'healthy'}), 200
    except:
        return jsonify({'status': 'unhealthy'}), 500
```

### 4. å‘Šè­¦é…ç½®

ä½¿ç”¨ç›‘æ§æœåŠ¡ï¼ˆå¦‚UptimeRobotï¼‰å®šæœŸæ£€æŸ¥ `/health` ç«¯ç‚¹ã€‚

---

## æ•…éšœæ’æŸ¥

### å¸¸è§é—®é¢˜

#### 1. æœåŠ¡æ— æ³•å¯åŠ¨

```bash
# æ£€æŸ¥æ—¥å¿—
sudo journalctl -u robotics-arxiv -n 50

# æ£€æŸ¥ç«¯å£å ç”¨
sudo netstat -tlnp | grep 5001

# æ£€æŸ¥æƒé™
ls -la papers.db
```

#### 2. æ•°æ®åº“é”å®š

```bash
# SQLiteæ•°æ®åº“é”å®šé€šå¸¸æ˜¯å¹¶å‘é—®é¢˜
# è§£å†³æ–¹æ¡ˆï¼šè¿ç§»åˆ°PostgreSQLæˆ–ä½¿ç”¨è¿æ¥æ± 
```

#### 3. å®šæ—¶ä»»åŠ¡ä¸æ‰§è¡Œ

```bash
# æ£€æŸ¥APScheduleræ—¥å¿—
grep -i scheduler /var/log/robotics-arxiv/error.log

# æ£€æŸ¥ç¯å¢ƒå˜é‡
echo $AUTO_FETCH_ENABLED
```

#### 4. å†…å­˜ä¸è¶³

```bash
# å‡å°‘Gunicorn workeræ•°é‡
workers = 2  # åœ¨gunicorn_config.pyä¸­
```

---

## éƒ¨ç½²æ£€æŸ¥æ¸…å•

- [ ] ä»£ç å·²æäº¤åˆ°Git
- [ ] ç¯å¢ƒå˜é‡å·²é…ç½®
- [ ] æ•°æ®åº“å·²åˆå§‹åŒ–
- [ ] Gunicornå·²å®‰è£…
- [ ] SystemdæœåŠ¡å·²é…ç½®
- [ ] Nginxå·²é…ç½®
- [ ] SSLè¯ä¹¦å·²é…ç½®
- [ ] é˜²ç«å¢™å·²é…ç½®
- [ ] å¤‡ä»½è„šæœ¬å·²è®¾ç½®
- [ ] ç›‘æ§å·²é…ç½®
- [ ] æ—¥å¿—ç›®å½•å·²åˆ›å»º
- [ ] æœåŠ¡å·²å¯åŠ¨å¹¶æµ‹è¯•

---

**æ–‡æ¡£åˆ›å»ºæ—¶é—´**: 2025-12-08  
**æœ€åæ›´æ–°æ—¶é—´**: 2025-12-08  
**æ–‡æ¡£ç»´æŠ¤è€…**: é¡¹ç›®å¼€å‘å›¢é˜Ÿ

