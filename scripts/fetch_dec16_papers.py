#!/usr/bin/env python3
"""
ä¸“é—¨æŠ“å–12æœˆ16æ—¥çš„è®ºæ–‡ï¼ˆæ‰‹åŠ¨æŠ“å–ï¼‰
"""
import sys
import os
from datetime import datetime, date
from pathlib import Path

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°è·¯å¾„
project_root = Path(__file__).parent.parent
sys.path.insert(0, str(project_root))

import arxiv
from models import get_session, Paper
# ä¸´æ—¶ç¦ç”¨reclassifyå¯¼å…¥ï¼Œé¿å…è¯­æ³•é”™è¯¯
import sys
original_path = sys.path.copy()
try:
    from save_paper_to_db import save_paper_to_db
except IndentationError:
    # å¦‚æœå¯¼å…¥å¤±è´¥ï¼Œç›´æ¥å®šä¹‰ç®€åŒ–ç‰ˆçš„ä¿å­˜å‡½æ•°
    def save_paper_to_db(paper_data, category, enable_title_dedup=True, fetch_semantic_scholar=False):
        from models import get_session, Paper
        from datetime import datetime
        session = get_session()
        try:
            paper_id = paper_data.get('id')
            if not paper_id:
                return False, 'error'
            
            # æ£€æŸ¥æ˜¯å¦å·²å­˜åœ¨
            existing = session.query(Paper).filter_by(id=paper_id).first()
            if existing:
                return True, 'skipped'
            
            # åˆ›å»ºæ–°è®°å½•
            paper = Paper(
                id=paper_id,
                title=paper_data.get('title', ''),
                authors=paper_data.get('authors', ''),
                abstract=paper_data.get('abstract', ''),
                pdf_url=paper_data.get('pdf_url', ''),
                code_url=paper_data.get('code_url'),
                category=category,
                publish_date=paper_data.get('publish_date'),
                update_date=paper_data.get('update_date')
            )
            session.add(paper)
            session.commit()
            return True, 'created'
        except Exception as e:
            session.rollback()
            print(f"ä¿å­˜å¤±è´¥: {e}")
            return False, 'error'
        finally:
            session.close()
from taxonomy import normalize_category
from sqlalchemy import func

def fetch_and_save_dec16_papers():
    """æŠ“å–å¹¶ä¿å­˜12æœˆ16æ—¥é¦–æ¬¡å‘å¸ƒçš„è®ºæ–‡"""
    print("=" * 60)
    print("å¼€å§‹æŠ“å–12æœˆ16æ—¥é¦–æ¬¡å‘å¸ƒçš„è®ºæ–‡...")
    print("=" * 60)
    
    target_date = date(2025, 12, 16)
    start_date_str = target_date.strftime('%Y%m%d')
    end_date_str = target_date.strftime('%Y%m%d')
    
    # æŸ¥è¯¢12æœˆ16æ—¥é¦–æ¬¡å‘å¸ƒçš„æ‰€æœ‰è®ºæ–‡
    query = f'submittedDate:[{start_date_str}0000 TO {end_date_str}2359]'
    
    print(f"æŸ¥è¯¢: {query}")
    print(f"ç›®æ ‡æ—¥æœŸ: {target_date}")
    print()
    
    client = arxiv.Client(
        page_size=100,
        delay_seconds=1.5,
        num_retries=3
    )
    
    search = arxiv.Search(
        query=query,
        max_results=200,  # 12æœˆ16æ—¥å¤§çº¦æœ‰50-100ç¯‡è®ºæ–‡
        sort_by=arxiv.SortCriterion.SubmittedDate,
        sort_order=arxiv.SortOrder.Descending
    )
    
    session = get_session()
    saved_count = 0
    skipped_count = 0
    error_count = 0
    
    print("å¼€å§‹æŠ“å–è®ºæ–‡...")
    print()
    
    try:
        results = list(client.results(search))
        print(f"ä»ArXiv APIè·å–åˆ° {len(results)} ç¯‡è®ºæ–‡")
        print()
        
        for i, result in enumerate(results, 1):
            try:
                paper_id = result.get_short_id()
                published_date = result.published.date()
                updated_date = result.updated.date()
                
                # åªå¤„ç†12æœˆ16æ—¥é¦–æ¬¡å‘å¸ƒçš„è®ºæ–‡
                if published_date != target_date:
                    continue
                
                # æ£€æŸ¥æ˜¯å¦å·²å­˜åœ¨
                existing = session.query(Paper).filter_by(id=paper_id).first()
                if existing:
                    print(f"[{i}/{len(results)}] è·³è¿‡å·²å­˜åœ¨: {paper_id} - {result.title[:50]}...")
                    skipped_count += 1
                    continue
                
                # æ„å»ºè®ºæ–‡æ•°æ®
                paper_data = {
                    'id': paper_id,
                    'title': result.title,
                    'authors': ', '.join([author.name for author in result.authors]),
                    'abstract': result.summary.replace('\n', ' '),
                    'pdf_url': result.pdf_url,
                    'code_url': None,  # ç¨åå¯ä»¥ä»commentsä¸­æå–
                    'date': published_date.strftime('%Y-%m-%d'),
                    'publish_date': published_date,
                    'update_date': updated_date
                }
                
                # å°è¯•ä»commentsä¸­æå–ä»£ç é“¾æ¥
                if result.comment:
                    import re
                    urls = re.findall(r'(https?://[^\s,;]+)', result.comment)
                    if urls:
                        paper_data['code_url'] = urls[0]
                
                # å°è¯•è‡ªåŠ¨åˆ†ç±»ï¼ˆåŸºäºæ ‡é¢˜å’Œæ‘˜è¦ï¼‰
                category = 'Uncategorized'
                title_lower = result.title.lower()
                abstract_lower = result.summary.lower()
                text = title_lower + ' ' + abstract_lower
                
                # ç®€å•çš„å…³é”®è¯åŒ¹é…åˆ†ç±»
                if any(kw in text for kw in ['perception', 'vision', 'visual', 'image', '2d', '3d']):
                    if '3d' in text or 'depth' in text or 'point cloud' in text:
                        category = 'Perception/3D Perception'
                    else:
                        category = 'Perception/2D Perception'
                elif any(kw in text for kw in ['robot', 'robotic', 'robotics']):
                    if any(kw in text for kw in ['humanoid', 'bipedal']):
                        category = 'Motion/Humanoid'
                    elif any(kw in text for kw in ['quadruped', 'legged']):
                        category = 'Motion/Quadruped'
                    elif any(kw in text for kw in ['manipulation', 'grasp', 'grasping']):
                        category = 'Operation/Grasp'
                    else:
                        category = 'General-Robot'
                elif any(kw in text for kw in ['vla', 'vision language action', 'embodied agent']):
                    category = 'Operation/VLA'
                elif any(kw in text for kw in ['reinforcement learning', 'rl', 'ppo', 'sac']):
                    category = 'Learning/RL'
                elif any(kw in text for kw in ['planning', 'navigation', 'path']):
                    category = 'Decision/Planning'
                
                # è§„èŒƒåŒ–ç±»åˆ«
                category = normalize_category(category)
                
                paper_data['category'] = category
                
                # ä¿å­˜åˆ°æ•°æ®åº“
                success, action = save_paper_to_db(
                    paper_data,
                    category,
                    enable_title_dedup=True,
                    fetch_semantic_scholar=False  # æš‚æ—¶ä¸è·å–Semantic Scholaræ•°æ®ï¼ŒåŠ å¿«é€Ÿåº¦
                )
                
                if success:
                    if action == 'created':
                        saved_count += 1
                        print(f"[{i}/{len(results)}] âœ… æ–°å¢: {paper_id} - {result.title[:50]}...")
                        print(f"     ç±»åˆ«: {category}, å‘å¸ƒæ—¥æœŸ: {published_date}")
                    else:
                        skipped_count += 1
                        print(f"[{i}/{len(results)}] â­ï¸  æ›´æ–°: {paper_id} - {result.title[:50]}...")
                else:
                    error_count += 1
                    print(f"[{i}/{len(results)}] âŒ ä¿å­˜å¤±è´¥: {paper_id} - {result.title[:50]}...")
                
            except Exception as e:
                error_count += 1
                print(f"[{i}/{len(results)}] âŒ å¤„ç†å¤±è´¥: {e}")
                import traceback
                traceback.print_exc()
                continue
        
        session.commit()
        
    except Exception as e:
        print(f"âŒ æŠ“å–å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
    finally:
        session.close()
    
    print()
    print("=" * 60)
    print("æŠ“å–å®Œæˆï¼")
    print("=" * 60)
    print(f"âœ… æ–°å¢: {saved_count} ç¯‡")
    print(f"â­ï¸  è·³è¿‡: {skipped_count} ç¯‡")
    print(f"âŒ é”™è¯¯: {error_count} ç¯‡")
    print()
    
    # éªŒè¯ç»“æœ
    session = get_session()
    dec16_count = session.query(func.count(Paper.id)).filter(
        func.date(Paper.publish_date) == target_date
    ).scalar()
    session.close()
    
    print(f"ğŸ“Š æ•°æ®åº“ä¸­12æœˆ16æ—¥çš„è®ºæ–‡æ€»æ•°: {dec16_count} ç¯‡")
    
    if dec16_count > 0:
        print("âœ… æˆåŠŸæŠ“å–åˆ°12æœˆ16æ—¥çš„è®ºæ–‡ï¼")
    else:
        print("âš ï¸  æ•°æ®åº“ä¸­ä»ç„¶æ²¡æœ‰12æœˆ16æ—¥çš„è®ºæ–‡")

if __name__ == '__main__':
    fetch_and_save_dec16_papers()

