#!/usr/bin/env python3
"""
ä¸“é—¨æŠ“å–æŒ‡å®šæ—¥æœŸçš„è®ºæ–‡ï¼ˆç”¨äºæµ‹è¯•ï¼‰
ç”¨æ³•: python3 scripts/fetch_specific_date.py --date 2025-12-16
"""
import sys
import os
from datetime import datetime, timedelta
from pathlib import Path

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°è·¯å¾„
project_root = Path(__file__).parent.parent
sys.path.insert(0, str(project_root))

import argparse
from daily_arxiv import load_config, demo
from models import get_session, Paper
from sqlalchemy import func

def fetch_papers_for_date(target_date_str):
    """
    æŠ“å–æŒ‡å®šæ—¥æœŸçš„è®ºæ–‡
    
    Args:
        target_date_str: æ—¥æœŸå­—ç¬¦ä¸²ï¼Œæ ¼å¼ï¼šYYYY-MM-DD
    """
    print("=" * 60)
    print(f"å¼€å§‹æŠ“å– {target_date_str} çš„è®ºæ–‡...")
    print("=" * 60)
    
    # è§£æç›®æ ‡æ—¥æœŸ
    try:
        target_date = datetime.strptime(target_date_str, '%Y-%m-%d').date()
    except ValueError:
        print(f"âŒ æ—¥æœŸæ ¼å¼é”™è¯¯ï¼Œè¯·ä½¿ç”¨ YYYY-MM-DD æ ¼å¼ï¼Œä¾‹å¦‚ï¼š2025-12-16")
        return
    
    print(f"ğŸ“… ç›®æ ‡æ—¥æœŸ: {target_date}")
    print(f"ğŸ“… ä»Šå¤©æ˜¯: {datetime.now().date()}")
    
    # æ£€æŸ¥æ•°æ®åº“ä¸­æ˜¯å¦å·²æœ‰è¯¥æ—¥æœŸçš„è®ºæ–‡
    session = get_session()
    existing_papers = session.query(Paper).filter(
        func.date(Paper.publish_date) == target_date
    ).all()
    print(f"ğŸ“Š æ•°æ®åº“ä¸­å·²æœ‰ {len(existing_papers)} ç¯‡ {target_date} çš„è®ºæ–‡")
    
    if len(existing_papers) > 0:
        print("\nå·²æœ‰è®ºæ–‡åˆ—è¡¨ï¼ˆå‰5ç¯‡ï¼‰ï¼š")
        for i, paper in enumerate(existing_papers[:5], 1):
            print(f"  {i}. {paper.title[:60]}...")
            print(f"     æäº¤æ—¥æœŸ: {paper.publish_date}, åˆ›å»ºæ—¶é—´: {paper.created_at}")
    
    session.close()
    
    # åŠ è½½é…ç½®
    config = load_config('config.yaml')
    config['max_results'] = 200  # å¢åŠ æŠ“å–æ•°é‡ï¼Œç¡®ä¿ä¸é—æ¼
    config['update_paper_links'] = False
    config['enable_dedup'] = True
    config['enable_incremental'] = True
    
    # è®¡ç®—æ—¥æœŸèŒƒå›´ï¼šç›®æ ‡æ—¥æœŸå‰åå„1å¤©ï¼Œç¡®ä¿èƒ½æŠ“åˆ°
    start_date = target_date - timedelta(days=1)
    end_date = target_date + timedelta(days=1)
    days_back = (datetime.now().date() - start_date).days + 1
    
    config['days_back'] = days_back
    config['fetch_semantic_scholar'] = True
    config['publish_gitpage'] = False
    config['publish_wechat'] = False
    
    print(f"\nğŸ“Š æŠ“å–é…ç½®:")
    print(f"  æ—¥æœŸèŒƒå›´: {start_date} åˆ° {end_date}")
    print(f"  days_back: {days_back} å¤©")
    print(f"  max_results: {config['max_results']}")
    
    # ä¿®æ”¹ daily_arxiv.py çš„æ—¥æœŸè¿‡æ»¤é€»è¾‘ï¼ˆä¸´æ—¶ï¼‰
    # æˆ‘ä»¬éœ€è¦åœ¨æŸ¥è¯¢ä¸­æ·»åŠ æ›´ç²¾ç¡®çš„æ—¥æœŸè¿‡æ»¤
    print(f"\nğŸ” å¼€å§‹ä»ArXiv APIæŠ“å–...")
    
    try:
        # è°ƒç”¨demoå‡½æ•°è¿›è¡ŒæŠ“å–
        demo(**config)
        
        # æŠ“å–å®Œæˆåï¼Œå†æ¬¡æ£€æŸ¥æ•°æ®åº“
        print("\n" + "=" * 60)
        print("æŠ“å–å®Œæˆï¼Œæ£€æŸ¥ç»“æœ...")
        print("=" * 60)
        
        session = get_session()
        new_papers = session.query(Paper).filter(
            func.date(Paper.publish_date) == target_date
        ).all()
        
        print(f"ğŸ“Š æ•°æ®åº“ä¸­ç°åœ¨æœ‰ {len(new_papers)} ç¯‡ {target_date} çš„è®ºæ–‡")
        
        if len(new_papers) > len(existing_papers):
            added_count = len(new_papers) - len(existing_papers)
            print(f"âœ… æˆåŠŸæ–°å¢ {added_count} ç¯‡è®ºæ–‡ï¼")
            
            print("\næ–°å¢è®ºæ–‡åˆ—è¡¨ï¼š")
            for i, paper in enumerate(new_papers[len(existing_papers):], 1):
                print(f"  {i}. {paper.title[:60]}...")
                print(f"     æäº¤æ—¥æœŸ: {paper.publish_date}, åˆ›å»ºæ—¶é—´: {paper.created_at}")
                print(f"     URL: {paper.url}")
        else:
            print("âš ï¸  æ²¡æœ‰æ–°å¢è®ºæ–‡")
            print("\nå¯èƒ½çš„åŸå› ï¼š")
            print("1. ArXiv APIåœ¨ç›®æ ‡æ—¥æœŸæ²¡æœ‰è¿”å›æ–°è®ºæ–‡")
            print("2. è®ºæ–‡çš„æäº¤æ—¥æœŸï¼ˆsubmittedDateï¼‰å’Œå‘å¸ƒæ—¥æœŸï¼ˆpublish_dateï¼‰ä¸åŒ")
            print("3. è®ºæ–‡å¯èƒ½åœ¨å…¶ä»–æ—¥æœŸæäº¤ï¼Œä½†æ˜¾ç¤ºä¸º12æœˆ16æ—¥")
        
        # æ£€æŸ¥æœ€è¿‘åˆ›å»ºçš„è®ºæ–‡ï¼ˆå¯èƒ½æ˜¯12æœˆ16æ—¥æäº¤çš„ï¼‰
        print("\n" + "=" * 60)
        print("æ£€æŸ¥æœ€è¿‘åˆ›å»ºçš„è®ºæ–‡ï¼ˆå¯èƒ½æ˜¯ç›®æ ‡æ—¥æœŸæäº¤çš„ï¼‰...")
        print("=" * 60)
        
        recent_papers = session.query(Paper).filter(
            Paper.created_at >= datetime.now() - timedelta(hours=1)
        ).order_by(Paper.created_at.desc()).limit(10).all()
        
        if recent_papers:
            print(f"ğŸ“Š æœ€è¿‘1å°æ—¶å†…åˆ›å»ºçš„è®ºæ–‡: {len(recent_papers)} ç¯‡")
            for i, paper in enumerate(recent_papers, 1):
                print(f"  {i}. {paper.title[:60]}...")
                print(f"     æäº¤æ—¥æœŸ: {paper.publish_date}, åˆ›å»ºæ—¶é—´: {paper.created_at}")
        else:
            print("âš ï¸  æœ€è¿‘1å°æ—¶å†…æ²¡æœ‰åˆ›å»ºæ–°è®ºæ–‡")
        
        session.close()
        
        print("\n" + "=" * 60)
        print("âœ… æŠ“å–ä»»åŠ¡å®Œæˆ")
        print("=" * 60)
        
    except Exception as e:
        print(f"\nâŒ æŠ“å–å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()

def main():
    parser = argparse.ArgumentParser(description='æŠ“å–æŒ‡å®šæ—¥æœŸçš„è®ºæ–‡')
    parser.add_argument('--date', type=str, required=True, 
                       help='ç›®æ ‡æ—¥æœŸï¼Œæ ¼å¼ï¼šYYYY-MM-DDï¼Œä¾‹å¦‚ï¼š2025-12-16')
    args = parser.parse_args()
    
    fetch_papers_for_date(args.date)

if __name__ == '__main__':
    main()

