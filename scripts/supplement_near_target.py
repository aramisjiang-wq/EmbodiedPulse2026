"""
é’ˆå¯¹æ¥è¿‘100ç¯‡çš„æ ‡ç­¾ï¼ˆ50-99ç¯‡ï¼‰è¿›è¡Œç²¾å‡†è¡¥å……
ç›®æ ‡ï¼šå¿«é€Ÿè®©è¿™äº›æ ‡ç­¾è¾¾åˆ°100ç¯‡ä»¥ä¸Š
"""
import sys
import os
sys.path.insert(0, os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

import arxiv
import logging
from datetime import datetime
from models import get_session, Paper
from taxonomy import normalize_category
from scripts.reclassify_all_papers import classify_paper_by_keywords
from sqlalchemy import func
import time

logging.basicConfig(
    format='[%(asctime)s %(levelname)s] %(message)s',
    datefmt='%Y-%m-%d %H:%M:%S',
    level=logging.INFO
)

# ä¸ºæ¥è¿‘è¾¾æ ‡çš„æ ‡ç­¾å®šä¹‰ç²¾å‡†æŸ¥è¯¢
# ç›®æ ‡ï¼šæ¯ä¸ªæ ‡ç­¾è‡³å°‘120ç¯‡ï¼ˆç•™æœ‰ä½™åœ°ï¼‰
NEAR_TARGET_QUERIES = [
    # 99ç¯‡ - åªå·®1ç¯‡ï¼
    ("Perception/Image Captioning", 120, [
        "ti:image captioning robot",
        "ti:visual captioning embodied",
        "ti:image description robot",
        "ti:visual description generation",
    ]),
    
    # 58ç¯‡ - å·®42ç¯‡
    ("Perception/Point Cloud", 120, [
        "ti:point cloud robot",
        "ti:lidar robot perception",
        "ti:pointnet manipulation",
        "ti:3d point robot",
        "ti:point cloud segmentation robot",
    ]),
    
    ("Motion Control/Motion Retargeting", 120, [
        "ti:motion retargeting",
        "ti:retarget motion robot",
        "ti:human motion robot",
        "ti:motion transfer humanoid",
        "ti:motion mapping",
    ]),
    
    # 56ç¯‡ - å·®44ç¯‡
    ("Perception/Generative Models", 120, [
        "ti:diffusion model robot",
        "ti:generative model manipulation",
        "ti:VAE robot",
        "ti:GAN robot",
        "ti:diffusion robot learning",
    ]),
    
    # 48ç¯‡ - å·®52ç¯‡
    ("Motion Control/Quadruped Robot", 120, [
        "ti:quadruped robot",
        "ti:four-legged robot",
        "ti:quadrupedal locomotion",
        "ti:four-leg robot",
    ]),
    
    ("Decision/Chain of Thought", 120, [
        "ti:chain-of-thought reasoning",
        "ti:CoT robot",
        "ti:step-by-step reasoning robot",
        "ti:reasoning chain embodied",
        "ti:thought process robot",
    ]),
    
    # 44ç¯‡ - å·®56ç¯‡
    ("Operation/Bimanual Manipulation", 120, [
        "ti:bimanual manipulation",
        "ti:dual-arm robot",
        "ti:two-arm manipulation",
        "ti:bi-manual robot",
        "ti:dual arm coordination",
    ]),
]

# é›¶è®ºæ–‡æ ‡ç­¾ - æ›´æ¿€è¿›çš„æŸ¥è¯¢
ZERO_TAG_QUERIES = [
    ("Decision/Historical Modeling", 120, [
        "ti:memory robot",
        "ti:episodic memory robot",
        "ti:experience replay robot",
        "ti:temporal robot",
        "ti:recurrent robot learning",
        "ti:LSTM robot",
    ]),
    
    ("Motion Control/Whole-Body Control", 120, [
        "ti:humanoid motion control",
        "ti:humanoid robot control",
        "ti:full-body humanoid",
        "ti:coordination humanoid",
    ]),
    
    ("Operation/Teleoperation", 120, [
        "ti:shared autonomy",
        "ti:haptic robot",
        "ti:VR robot control",
        "ti:virtual reality manipulation",
        "ti:human-in-the-loop robot",
    ]),
]


def fetch_for_tag(tag_key, target_count, queries):
    """ä¸ºæŒ‡å®šæ ‡ç­¾æŠ“å–è®ºæ–‡"""
    session = get_session()
    
    current_count = session.query(func.count(Paper.id)).filter(
        Paper.category == tag_key
    ).scalar()
    
    logging.info(f"{'='*70}")
    logging.info(f"ğŸ“Œ æ ‡ç­¾: {tag_key}")
    logging.info(f"   å½“å‰: {current_count}ç¯‡ | ç›®æ ‡: {target_count}ç¯‡ | éœ€è¦: {target_count - current_count}ç¯‡")
    logging.info(f"{'='*70}")
    
    if current_count >= target_count:
        logging.info(f"âœ… å·²è¾¾æ ‡ï¼Œè·³è¿‡")
        session.close()
        return 0
    
    all_papers = {}
    reclassified = 0
    
    for query in queries:
        logging.info(f"\nğŸ” æŸ¥è¯¢: {query}")
        
        try:
            search = arxiv.Search(
                query=query,
                max_results=200,
                sort_by=arxiv.SortCriterion.Relevance
            )
            
            fetched = 0
            for result in search.results():
                arxiv_id = result.entry_id.split('/abs/')[-1].split('v')[0]
                
                # æ£€æŸ¥å·²å­˜åœ¨
                existing = session.query(Paper).filter_by(id=arxiv_id).first()
                
                if existing:
                    # é‡æ–°åˆ†ç±»æœªåˆ†ç±»è®ºæ–‡
                    if existing.category == 'Uncategorized':
                        tags = classify_paper_by_keywords(existing)
                        if tag_key in tags:
                            existing.category = tag_key
                            existing.updated_at = datetime.now()
                            reclassified += 1
                            logging.info(f"   âœ“ é‡æ–°åˆ†ç±»: {arxiv_id}")
                else:
                    if arxiv_id not in all_papers:
                        all_papers[arxiv_id] = result
                        fetched += 1
                
                if fetched >= 50:
                    break
            
            logging.info(f"   æ‰¾åˆ°: {fetched}ç¯‡æ–°è®ºæ–‡")
            
            # æ£€æŸ¥æ˜¯å¦å·²è¾¾æ ‡
            current = session.query(func.count(Paper.id)).filter(
                Paper.category == tag_key
            ).scalar()
            if current >= target_count:
                logging.info(f"âœ… å·²è¾¾åˆ°ç›®æ ‡ {target_count}ç¯‡ï¼Œåœæ­¢æŠ“å–")
                session.commit()
                session.close()
                return reclassified
            
            time.sleep(3)
            
        except Exception as e:
            logging.error(f"   âŒ æŸ¥è¯¢å¤±è´¥: {e}")
            continue
    
    logging.info(f"\nğŸ“¦ æ–°è®ºæ–‡æ€»æ•°: {len(all_papers)}ç¯‡")
    
    # ä¿å­˜æ–°è®ºæ–‡
    saved_count = 0
    for arxiv_id, result in all_papers.items():
        try:
            temp_paper = Paper(
                id=arxiv_id,
                title=result.title,
                abstract=result.summary,
                publish_date=result.published.date()
            )
            
            tags = classify_paper_by_keywords(temp_paper)
            category = tags[0] if tags else 'Uncategorized'
            
            paper = Paper(
                id=arxiv_id,
                title=result.title,
                authors=', '.join([author.name for author in result.authors]),
                abstract=result.summary,
                publish_date=result.published.date(),
                pdf_url=result.pdf_url,
                category=category,
                created_at=datetime.now(),
                updated_at=datetime.now()
            )
            session.add(paper)
            
            if category == tag_key:
                saved_count += 1
            
            if saved_count % 50 == 0 and saved_count > 0:
                session.commit()
                
        except Exception as e:
            logging.error(f"   âŒ ä¿å­˜å¤±è´¥ {arxiv_id}: {e}")
            continue
    
    session.commit()
    
    # æœ€ç»ˆç»Ÿè®¡
    final_count = session.query(func.count(Paper.id)).filter(
        Paper.category == tag_key
    ).scalar()
    
    total_added = reclassified + saved_count
    status = "âœ…" if final_count >= 100 else "âš ï¸"
    
    logging.info(f"\n{'='*70}")
    logging.info(f"{status} å®Œæˆ: {tag_key}")
    logging.info(f"   æœ€ç»ˆ: {final_count}ç¯‡ (æ–°å¢: {total_added}ç¯‡)")
    logging.info(f"   - é‡æ–°åˆ†ç±»: {reclassified}ç¯‡")
    logging.info(f"   - æ–°æŠ“å–åˆ†ç±»åˆ°æ­¤: {saved_count}ç¯‡")
    logging.info(f"{'='*70}\n")
    
    session.close()
    return total_added


def main():
    """ä¸»å‡½æ•°"""
    logging.info("="*70)
    logging.info("ğŸš€ æ–¹æ¡ˆ1: å¿«é€Ÿè¾¾åˆ°25ä¸ªæ ‡ç­¾è¾¾æ ‡")
    logging.info("="*70)
    logging.info("ç¬¬ä¸€æ­¥: å¤„ç†7ä¸ªæ¥è¿‘è¾¾æ ‡çš„æ ‡ç­¾ (50-99ç¯‡)")
    logging.info("ç¬¬äºŒæ­¥: å¤„ç†3ä¸ªé›¶è®ºæ–‡æ ‡ç­¾")
    logging.info("="*70 + "\n")
    
    total_added = 0
    
    # ç¬¬ä¸€æ­¥ï¼šæ¥è¿‘è¾¾æ ‡çš„æ ‡ç­¾
    logging.info("\n" + "="*70)
    logging.info("ğŸ“Š ç¬¬ä¸€æ­¥: è¡¥å……æ¥è¿‘è¾¾æ ‡çš„æ ‡ç­¾")
    logging.info("="*70)
    
    for tag_key, target, queries in NEAR_TARGET_QUERIES:
        added = fetch_for_tag(tag_key, target, queries)
        total_added += added
        time.sleep(5)
    
    # ç¬¬äºŒæ­¥ï¼šé›¶è®ºæ–‡æ ‡ç­¾
    logging.info("\n" + "="*70)
    logging.info("ğŸ“Š ç¬¬äºŒæ­¥: è¡¥å……é›¶è®ºæ–‡æ ‡ç­¾")
    logging.info("="*70)
    
    for tag_key, target, queries in ZERO_TAG_QUERIES:
        added = fetch_for_tag(tag_key, target, queries)
        total_added += added
        time.sleep(5)
    
    # æœ€ç»ˆç»Ÿè®¡
    logging.info("\n" + "="*70)
    logging.info("âœ… è¡¥å……å®Œæˆï¼")
    logging.info(f"   æ€»å…±æ–°å¢: {total_added}ç¯‡è®ºæ–‡")
    logging.info("="*70)
    
    # ç»Ÿè®¡è¾¾æ ‡æƒ…å†µ
    from taxonomy import NEW_TAXONOMY
    session = get_session()
    stats = session.query(Paper.category, func.count(Paper.id)).group_by(
        Paper.category
    ).all()
    stats_dict = dict(stats)
    
    è¾¾æ ‡æ•° = sum(1 for tag_key in NEW_TAXONOMY.keys() 
                if stats_dict.get(tag_key, 0) >= 100)
    
    logging.info(f"\nğŸ“ˆ å½“å‰è¾¾æ ‡æ ‡ç­¾æ•°: {è¾¾æ ‡æ•°}/33")
    logging.info("="*70)
    
    session.close()


if __name__ == "__main__":
    main()
